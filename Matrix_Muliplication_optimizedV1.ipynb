{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzSmk8dqUQza2mS7tJ3qRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laere11/Laere11/blob/Mathematics/Matrix_Muliplication_optimizedV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simulation of the AI coâ€‘scientist process applied to your idea of blending Tucker decomposition, insights from AlphaTensor, and a reinforcement learning strategy to dynamically adapt the depth of matrix decomposition. The goal is to minimize the number of multiplication steps while ensuring that key submatrices that preserve accuracy are identified and maintained at a higher resolution.\n",
        "\n",
        "Generation Agent\n",
        "Hypothesis 1: Uniform Deep Tucker Decomposition with Postâ€‘Processing Sensitivity Analysis\n",
        "\n",
        "Core Idea:\n",
        "Initially decompose the matrix extensively using Tucker decompositionâ€”even if it means heavy subdivision into a small core tensor and associated factor matrices.\n",
        "Use a machine learning (ML) algorithm to evaluate the accuracy contribution of each decomposed submatrix (or module).\n",
        "How It Addresses the Problem:\n",
        "By treating every region equally at first, the approach sacrifices efficiency to obtain maximum resolution.\n",
        "A sensitivity analysis (using techniques analogous to feature importance in ML) then determines which submatrices are critical for maintaining the overall matrix product accuracy.\n",
        "Once key areas are identified, only those regions are â€œreconstructedâ€ with less decomposition (i.e., a shallower Tucker factorization), while less critical areas remain highly decomposed, thereby reducing multiplication counts where possible.\n",
        "Hypothesis 2: Adaptive Tucker-AlphaTensor Hybrid via Reinforcement Learning\n",
        "\n",
        "Core Idea:\n",
        "Extend the uniform decomposition by incorporating reinforcement learning (RL) to adapt the level of decomposition for each submatrix during the multiplication process.\n",
        "Inspired by AlphaTensor, the RL agent iteratively explores various decomposition depths and monitors an accuracy metric (relative to known truth or a user-provided validation criterion).\n",
        "How It Addresses the Problem:\n",
        "The RL agent assigns a â€œweightâ€ or importance score to each decomposed module by simulating different levels of decomposition and receiving feedback on accuracy losses.\n",
        "Over multiple iterations, the algorithm learns which blocks require minimal decomposition (to preserve accuracy) and which can be aggressively compressed.\n",
        "This adaptive strategy aims to blend the benefits of deep decomposition (for efficiency) with localized reconstruction (for accuracy), ultimately reducing the overall multiplication steps.\n",
        "Hypothesis 3: Multi-Level Modular Decomposition with Recursive Self-Assessment\n",
        "\n",
        "Core Idea:\n",
        "Break the matrix into modules via a recursive Tucker decompositionâ€”first at a coarse level, then further decomposing only the modules that pass a sensitivity threshold.\n",
        "Introduce a recursive feedback loop (via a meta-learning component) that reassesses the decomposition at each level based on an accuracy metric.\n",
        "How It Addresses the Problem:\n",
        "By recursively applying the decomposition only to modules that are â€œless criticalâ€ (i.e., have lower sensitivity scores), the algorithm limits the computational overhead.\n",
        "Modules critical for high accuracy are left closer to the original structure, ensuring that the overall multiplication is not compromised.\n",
        "This selective refinement reduces unnecessary multiplication steps by avoiding over-decomposition in key areas.\n",
        "Reflection Agent\n",
        "For Hypothesis 1 (Uniform Deep Tucker + Sensitivity Analysis):\n",
        "\n",
        "Strengths:\n",
        "Simple to implement initially; treats the whole matrix uniformly, which may capture hidden structure.\n",
        "Sensitivity analysis can highlight critical areas based on well-understood ML techniques (e.g., gradient-based importance, feature weighting).\n",
        "Weaknesses:\n",
        "Heavy initial decomposition could be extremely inefficient if the matrix is large.\n",
        "Post-processing may not perfectly identify critical modules if the initial decomposition loses too much detail.\n",
        "For Hypothesis 2 (Adaptive Tuckerâ€‘AlphaTensor Hybrid):\n",
        "\n",
        "Strengths:\n",
        "Integrates an RL agent to explore the trade-off between accuracy and efficiency dynamically.\n",
        "Inspired by AlphaTensorâ€™s success, it can discover nonintuitive decomposition patterns that humans might miss.\n",
        "Weaknesses:\n",
        "Requires a well-designed reward function and extensive training iterations, which might be computationally expensive.\n",
        "The exploration space is very large; convergence could be challenging.\n",
        "For Hypothesis 3 (Multi-Level Modular Decomposition):\n",
        "\n",
        "Strengths:\n",
        "Recursive approach allows for selective refinement and could balance decomposition depth adaptively.\n",
        "May lead to a more interpretable hierarchy of importance scores across different levels of the matrix.\n",
        "Weaknesses:\n",
        "Complexity increases with each recursion level, and errors might propagate if early assessments are off.\n",
        "Designing the recursive feedback loop could be intricate, requiring careful tuning.\n",
        "Ranking Agent\n",
        "Comparing the three approaches on theoretical efficiency gains and potential for reduced multiplication steps:\n",
        "\n",
        "Hypothesis 2 (Adaptive Tuckerâ€‘AlphaTensor Hybrid) emerges as the most promising since it leverages reinforcement learning to dynamically adjust the decomposition depth, akin to how AlphaTensor discovered novel algorithms.\n",
        "Hypothesis 3 (Multi-Level Modular Decomposition) is a close second, offering an elegant recursive strategy that could yield interpretable module importance but may suffer from cumulative errors.\n",
        "Hypothesis 1 (Uniform Deep Tucker + Sensitivity Analysis), while conceptually straightforward, may be less efficient due to its blanket initial decomposition.\n",
        "Evolution Agent\n",
        "Refined Algorithm Outline (Based on Hypothesis 2):\n",
        "\n",
        "Initialization:\n",
        "\n",
        "Perform an initial Tucker decomposition on the matrix\n",
        "ð‘€\n",
        "M into a core tensor\n",
        "ðº\n",
        "G and factor matrices\n",
        "ð´\n",
        ",\n",
        "ðµ\n",
        ",\n",
        "ð¶\n",
        "A,B,C (if working with a three-way formulation, or extend as needed).\n",
        "Partition the matrix (or its tensor representation) into multiple submodules corresponding to blocks of the core tensor.\n",
        "Reinforcement Learning Module:\n",
        "\n",
        "State Representation:\n",
        "Each submodule is characterized by its current decomposition depth, estimated reconstruction error, and a computed sensitivity metric (initially estimated via standard error analysis).\n",
        "Action Space:\n",
        "The RL agent can choose to increase, decrease, or maintain the decomposition level for each submodule.\n",
        "Reward Function:\n",
        "Define a reward that balances reduced multiplication count (favoring deeper decomposition) against a penalty for increased reconstruction error relative to a baseline accuracy metric. For example,\n",
        "Reward\n",
        "=\n",
        "âˆ’\n",
        "ð›¼\n",
        "Ã—\n",
        "(\n",
        "MultiplicationÂ Cost\n",
        ")\n",
        "âˆ’\n",
        "ð›½\n",
        "Ã—\n",
        "(\n",
        "ReconstructionÂ Error\n",
        ")\n",
        "Reward=âˆ’Î±Ã—(MultiplicationÂ Cost)âˆ’Î²Ã—(ReconstructionÂ Error)\n",
        "where\n",
        "ð›¼\n",
        "Î± and\n",
        "ð›½\n",
        "Î² are tunable weights.\n",
        "Policy Update:\n",
        "Use an RL algorithm (such as Q-learning or policy gradients) to iteratively update the policy that assigns the optimal decomposition level to each submodule.\n",
        "Iterative Feedback Loop:\n",
        "\n",
        "Run the RL agent over multiple episodes, each time decomposing, reconstructing, and evaluating the matrix multiplication result against known benchmarks or simulation of ground truth.\n",
        "Update sensitivity scores based on how changes in decomposition affect the overall accuracy. Critical modules (those whose under-decomposition causes large errors) are flagged to be maintained at shallower depths in subsequent iterations.\n",
        "Final Reconstruction:\n",
        "\n",
        "For modules deemed critical, reconstruct them with less aggressive decomposition (or even revert to a classical multiplication method).\n",
        "For modules with low sensitivity, retain the higher-level decomposition, thereby saving multiplication steps.\n",
        "Assemble the final output from both sets of modules.\n",
        "Output and Validation:\n",
        "\n",
        "Compute the overall multiplication result and compare it with a high-accuracy reference.\n",
        "Report both the multiplication count reduction (theoretical savings) and the overall accuracy achieved.\n",
        "Optionally, solicit user feedback on whether the accuracy meets the threshold, adjusting the RL reward parameters accordingly.\n",
        "User Interaction\n",
        "Before moving further with detailed pseudo-code or simulation experiments, here are a few clarifying questions:\n",
        "\n",
        "Scope of Decomposition:\n",
        "\n",
        "Are you envisioning applying this approach to dense matrices in general, or do you have specific structured matrices in mind (e.g., those arising in signal processing or machine learning)?\n",
        "Accuracy Thresholds:\n",
        "\n",
        "What level of reconstruction error or accuracy loss is acceptable in your target application?\n",
        "Would you like to specify a target metric (e.g., relative error below 1%) that the RL agent should aim to maintain?\n",
        "RL Framework Preferences:\n",
        "\n",
        "Do you have a preferred reinforcement learning algorithm (Q-learning, policy gradient, etc.), or should the design remain agnostic to the RL method initially?\n",
        "Feedback Mechanism:\n",
        "\n",
        "Should the feedback loop rely solely on computed error metrics (from a known ground truth), or do you want to incorporate human-in-the-loop adjustments during training?\n",
        "Your answers will help refine this algorithm further. This proposal outlines a theoretical framework that uses deep tensor decomposition (via Tucker) combined with an RL agent inspired by AlphaTensorâ€™s exploratory methods to adaptively optimize the trade-off between multiplication efficiency and accuracy.\n",
        "\n",
        "CODE -\n",
        "Below is a simplified Python implementation that simulates our proposed reinforcementâ€learningâ€“inspired strategy. In this example, we start with a structured matrix (here generated as a Vandermonde matrix for polynomial fitting) and partition it into blocks. Each block is initially approximated via a Tucker-like (SVD-based) decomposition with a low rank (deep decomposition). In each iteration, we compare the reconstruction error of each block to a preset threshold and adjust the â€œdecomposition levelâ€ (i.e. the rank) accordingly. This simulates our idea of â€œlearningâ€ which submatrices are critical for maintaining accuracy so that in subsequent loops those areas are decomposed less (using higher ranks)."
      ],
      "metadata": {
        "id": "LyVcKU3IrU0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper function: Compute truncated SVD reconstruction and relative error.\n",
        "def truncated_svd_reconstruction(block, rank):\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    # Truncate to desired rank (cannot exceed min(block dimensions))\n",
        "    r = min(rank, len(S))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    # Reconstruct approximation\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    # Compute relative Frobenius norm error\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error\n",
        "\n",
        "# Parameters\n",
        "n = 16  # matrix size\n",
        "num_blocks_per_dim = 2  # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "initial_rank = 2\n",
        "error_threshold = 0.1  # 10% relative error allowed initially\n",
        "num_iterations = 5\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "# Using np.vander creates a matrix with columns [x^(n-1), x^(n-2), ..., 1]\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks and initialize a rank matrix\n",
        "ranks = np.full((num_blocks_per_dim, num_blocks_per_dim), initial_rank, dtype=int)\n",
        "\n",
        "# Function to compute approximate multiplication cost for a block using its decomposition\n",
        "def multiplication_cost(b, r):\n",
        "    # Approximate cost: cost for multiplying b x r and r x b matrices (ignoring SVD overhead)\n",
        "    return b * b * r\n",
        "\n",
        "print(\"Initial Structured Matrix (M):\")\n",
        "print(M)\n",
        "print(\"\\nInitial block ranks:\")\n",
        "print(ranks)\n",
        "print(\"\\nStarting iterative refinement...\\n\")\n",
        "\n",
        "# Iterative refinement loop (simulating the RL feedback loop)\n",
        "for it in range(num_iterations):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    new_ranks = ranks.copy()\n",
        "    print(f\"Iteration {it+1}:\")\n",
        "\n",
        "    # Loop over blocks\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            # Extract block from M\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            current_rank = ranks[i, j]\n",
        "            # Get truncated SVD reconstruction and error\n",
        "            _, error = truncated_svd_reconstruction(block, current_rank)\n",
        "            total_error += error\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "\n",
        "            print(f\"  Block ({i},{j}): rank={current_rank}, rel_error={error:.4f}, cost~{cost}\")\n",
        "\n",
        "            # Update rule:\n",
        "            # If error exceeds threshold, use a higher rank next time (less deep decomposition)\n",
        "            # If error is well below threshold, we can try to lower the rank to reduce multiplications.\n",
        "            if error > error_threshold:\n",
        "                new_ranks[i, j] = current_rank + 1  # reduce decomposition (increase rank)\n",
        "            elif error < error_threshold * 0.5 and current_rank > 1:\n",
        "                new_ranks[i, j] = current_rank - 1  # can afford more decomposition (reduce rank)\n",
        "            else:\n",
        "                new_ranks[i, j] = current_rank  # keep as is\n",
        "\n",
        "    ranks = new_ranks\n",
        "    avg_error = total_error / (num_blocks_per_dim * num_blocks_per_dim)\n",
        "    print(f\"  Average relative error: {avg_error:.4f}\")\n",
        "    print(f\"  Total approximate multiplication cost for all blocks: {total_cost}\")\n",
        "    print(\"  Updated ranks:\")\n",
        "    print(ranks)\n",
        "    print(\"-\"*50)\n",
        "\n",
        "# Final output: reconstructed matrix from blocks (for demonstration)\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        r = ranks[i, j]\n",
        "        block_approx, _ = truncated_svd_reconstruction(block, r)\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KprVLWT7t_Gr",
        "outputId": "20a3abb8-8cb7-4ecb-b824-9eef9503e6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Structured Matrix (M):\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            " [2.28365826e-18 3.42548739e-17 5.13823109e-16 7.70734663e-15\n",
            "  1.15610199e-13 1.73415299e-12 2.60122949e-11 3.90184423e-10\n",
            "  5.85276635e-09 8.77914952e-08 1.31687243e-06 1.97530864e-05\n",
            "  2.96296296e-04 4.44444444e-03 6.66666667e-02 1.00000000e+00]\n",
            " [7.48309139e-14 5.61231854e-13 4.20923891e-12 3.15692918e-11\n",
            "  2.36769688e-10 1.77577266e-09 1.33182950e-08 9.98872123e-08\n",
            "  7.49154092e-07 5.61865569e-06 4.21399177e-05 3.16049383e-04\n",
            "  2.37037037e-03 1.77777778e-02 1.33333333e-01 1.00000000e+00]\n",
            " [3.27680000e-11 1.63840000e-10 8.19200000e-10 4.09600000e-09\n",
            "  2.04800000e-08 1.02400000e-07 5.12000000e-07 2.56000000e-06\n",
            "  1.28000000e-05 6.40000000e-05 3.20000000e-04 1.60000000e-03\n",
            "  8.00000000e-03 4.00000000e-02 2.00000000e-01 1.00000000e+00]\n",
            " [2.45205939e-09 9.19522270e-09 3.44820851e-08 1.29307819e-07\n",
            "  4.84904322e-07 1.81839121e-06 6.81896703e-06 2.55711264e-05\n",
            "  9.58917238e-05 3.59593964e-04 1.34847737e-03 5.05679012e-03\n",
            "  1.89629630e-02 7.11111111e-02 2.66666667e-01 1.00000000e+00]\n",
            " [6.96917194e-08 2.09075158e-07 6.27225474e-07 1.88167642e-06\n",
            "  5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
            "  4.57247371e-04 1.37174211e-03 4.11522634e-03 1.23456790e-02\n",
            "  3.70370370e-02 1.11111111e-01 3.33333333e-01 1.00000000e+00]\n",
            " [1.07374182e-06 2.68435456e-06 6.71088640e-06 1.67772160e-05\n",
            "  4.19430400e-05 1.04857600e-04 2.62144000e-04 6.55360000e-04\n",
            "  1.63840000e-03 4.09600000e-03 1.02400000e-02 2.56000000e-02\n",
            "  6.40000000e-02 1.60000000e-01 4.00000000e-01 1.00000000e+00]\n",
            " [1.08418081e-05 2.32324458e-05 4.97838125e-05 1.06679598e-04\n",
            "  2.28599139e-04 4.89855298e-04 1.04968992e-03 2.24933555e-03\n",
            "  4.82000476e-03 1.03285816e-02 2.21326749e-02 4.74271605e-02\n",
            "  1.01629630e-01 2.17777778e-01 4.66666667e-01 1.00000000e+00]\n",
            " [8.03490820e-05 1.50654529e-04 2.82477241e-04 5.29644827e-04\n",
            "  9.93084051e-04 1.86203260e-03 3.49131112e-03 6.54620835e-03\n",
            "  1.22741406e-02 2.30140137e-02 4.31512757e-02 8.09086420e-02\n",
            "  1.51703704e-01 2.84444444e-01 5.33333333e-01 1.00000000e+00]\n",
            " [4.70184985e-04 7.83641641e-04 1.30606940e-03 2.17678234e-03\n",
            "  3.62797056e-03 6.04661760e-03 1.00776960e-02 1.67961600e-02\n",
            "  2.79936000e-02 4.66560000e-02 7.77600000e-02 1.29600000e-01\n",
            "  2.16000000e-01 3.60000000e-01 6.00000000e-01 1.00000000e+00]\n",
            " [2.28365826e-03 3.42548739e-03 5.13823109e-03 7.70734663e-03\n",
            "  1.15610199e-02 1.73415299e-02 2.60122949e-02 3.90184423e-02\n",
            "  5.85276635e-02 8.77914952e-02 1.31687243e-01 1.97530864e-01\n",
            "  2.96296296e-01 4.44444444e-01 6.66666667e-01 1.00000000e+00]\n",
            " [9.53940729e-03 1.30082827e-02 1.77385673e-02 2.41889554e-02\n",
            "  3.29849391e-02 4.49794625e-02 6.13356306e-02 8.36394963e-02\n",
            "  1.14053859e-01 1.55527989e-01 2.12083621e-01 2.89204938e-01\n",
            "  3.94370370e-01 5.37777778e-01 7.33333333e-01 1.00000000e+00]\n",
            " [3.51843721e-02 4.39804651e-02 5.49755814e-02 6.87194767e-02\n",
            "  8.58993459e-02 1.07374182e-01 1.34217728e-01 1.67772160e-01\n",
            "  2.09715200e-01 2.62144000e-01 3.27680000e-01 4.09600000e-01\n",
            "  5.12000000e-01 6.40000000e-01 8.00000000e-01 1.00000000e+00]\n",
            " [1.16891087e-01 1.34874332e-01 1.55624229e-01 1.79566418e-01\n",
            "  2.07192021e-01 2.39067716e-01 2.75847365e-01 3.18285421e-01\n",
            "  3.67252409e-01 4.23752779e-01 4.88945514e-01 5.64167901e-01\n",
            "  6.50962963e-01 7.51111111e-01 8.66666667e-01 1.00000000e+00]\n",
            " [3.55264366e-01 3.80640393e-01 4.07828992e-01 4.36959634e-01\n",
            "  4.68171037e-01 5.01611825e-01 5.37441241e-01 5.75829901e-01\n",
            "  6.16960609e-01 6.61029224e-01 7.08245597e-01 7.58834568e-01\n",
            "  8.13037037e-01 8.71111111e-01 9.33333333e-01 1.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]]\n",
            "\n",
            "Initial block ranks:\n",
            "[[2 2]\n",
            " [2 2]]\n",
            "\n",
            "Starting iterative refinement...\n",
            "\n",
            "Iteration 1:\n",
            "  Block (0,0): rank=2, rel_error=0.0007, cost~128\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=2, rel_error=0.0069, cost~128\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0188\n",
            "  Total approximate multiplication cost for all blocks: 512\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "Iteration 2:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=1, rel_error=0.1618, cost~64\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=1, rel_error=0.2863, cost~64\n",
            "  Average relative error: 0.1402\n",
            "  Total approximate multiplication cost for all blocks: 256\n",
            "  Updated ranks:\n",
            "[[1 2]\n",
            " [1 2]]\n",
            "--------------------------------------------------\n",
            "Iteration 3:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0450\n",
            "  Total approximate multiplication cost for all blocks: 384\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "Iteration 4:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=1, rel_error=0.1618, cost~64\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=1, rel_error=0.2863, cost~64\n",
            "  Average relative error: 0.1402\n",
            "  Total approximate multiplication cost for all blocks: 256\n",
            "  Updated ranks:\n",
            "[[1 2]\n",
            " [1 2]]\n",
            "--------------------------------------------------\n",
            "Iteration 5:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0450\n",
            "  Total approximate multiplication cost for all blocks: 384\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Final overall relative reconstruction error: 0.2336335896759094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT Explanation**\n",
        "Matrix Generation & Partitioning:\n",
        "\n",
        "We generate a 16Ã—16 Vandermonde matrix to simulate a structured matrix used in polynomial fitting.\n",
        "The matrix is partitioned into 4 blocks (2Ã—2 grid), and each block is assigned an initial decomposition rank (here 2).\n",
        "Truncated SVD as Tucker-Like Decomposition:\n",
        "\n",
        "For each block, we perform a truncated SVD (our surrogate for Tucker decomposition) to approximate the block using the specified rank.\n",
        "We compute the relative Frobenius norm error as a measure of reconstruction accuracy.\n",
        "Iterative Refinement (Simulated RL Loop):\n",
        "\n",
        "In each iteration, the algorithm checks the error of each block.\n",
        "If the error exceeds the threshold, the rank is increased for that block (less aggressive decomposition) to preserve accuracy.\n",
        "If the error is well below the threshold, the rank is decreased (allowing more aggressive decomposition to reduce multiplication cost).\n",
        "A simple cost model based on block size and rank is computed as a proxy for multiplication steps.\n",
        "Final Reconstruction:\n",
        "\n",
        "After several iterations, each block is reconstructed using its updated rank, and the overall reconstruction error of the entire matrix is reported.\n",
        "This code represents a first simplified proof-of-concept that you can further extend and refine. It implements a basic reinforcement learningâ€“inspired adjustment (using rule-based updates) with a known ground truth (the original matrix) as feedback."
      ],
      "metadata": {
        "id": "9zt0U9wvryYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rVREAxrs5qy",
        "outputId": "a664c06e-bf2e-4dda-9c87-7175160f0787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Structured Matrix (M):\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            " [2.28365826e-18 3.42548739e-17 5.13823109e-16 7.70734663e-15\n",
            "  1.15610199e-13 1.73415299e-12 2.60122949e-11 3.90184423e-10\n",
            "  5.85276635e-09 8.77914952e-08 1.31687243e-06 1.97530864e-05\n",
            "  2.96296296e-04 4.44444444e-03 6.66666667e-02 1.00000000e+00]\n",
            " [7.48309139e-14 5.61231854e-13 4.20923891e-12 3.15692918e-11\n",
            "  2.36769688e-10 1.77577266e-09 1.33182950e-08 9.98872123e-08\n",
            "  7.49154092e-07 5.61865569e-06 4.21399177e-05 3.16049383e-04\n",
            "  2.37037037e-03 1.77777778e-02 1.33333333e-01 1.00000000e+00]\n",
            " [3.27680000e-11 1.63840000e-10 8.19200000e-10 4.09600000e-09\n",
            "  2.04800000e-08 1.02400000e-07 5.12000000e-07 2.56000000e-06\n",
            "  1.28000000e-05 6.40000000e-05 3.20000000e-04 1.60000000e-03\n",
            "  8.00000000e-03 4.00000000e-02 2.00000000e-01 1.00000000e+00]\n",
            " [2.45205939e-09 9.19522270e-09 3.44820851e-08 1.29307819e-07\n",
            "  4.84904322e-07 1.81839121e-06 6.81896703e-06 2.55711264e-05\n",
            "  9.58917238e-05 3.59593964e-04 1.34847737e-03 5.05679012e-03\n",
            "  1.89629630e-02 7.11111111e-02 2.66666667e-01 1.00000000e+00]\n",
            " [6.96917194e-08 2.09075158e-07 6.27225474e-07 1.88167642e-06\n",
            "  5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
            "  4.57247371e-04 1.37174211e-03 4.11522634e-03 1.23456790e-02\n",
            "  3.70370370e-02 1.11111111e-01 3.33333333e-01 1.00000000e+00]\n",
            " [1.07374182e-06 2.68435456e-06 6.71088640e-06 1.67772160e-05\n",
            "  4.19430400e-05 1.04857600e-04 2.62144000e-04 6.55360000e-04\n",
            "  1.63840000e-03 4.09600000e-03 1.02400000e-02 2.56000000e-02\n",
            "  6.40000000e-02 1.60000000e-01 4.00000000e-01 1.00000000e+00]\n",
            " [1.08418081e-05 2.32324458e-05 4.97838125e-05 1.06679598e-04\n",
            "  2.28599139e-04 4.89855298e-04 1.04968992e-03 2.24933555e-03\n",
            "  4.82000476e-03 1.03285816e-02 2.21326749e-02 4.74271605e-02\n",
            "  1.01629630e-01 2.17777778e-01 4.66666667e-01 1.00000000e+00]\n",
            " [8.03490820e-05 1.50654529e-04 2.82477241e-04 5.29644827e-04\n",
            "  9.93084051e-04 1.86203260e-03 3.49131112e-03 6.54620835e-03\n",
            "  1.22741406e-02 2.30140137e-02 4.31512757e-02 8.09086420e-02\n",
            "  1.51703704e-01 2.84444444e-01 5.33333333e-01 1.00000000e+00]\n",
            " [4.70184985e-04 7.83641641e-04 1.30606940e-03 2.17678234e-03\n",
            "  3.62797056e-03 6.04661760e-03 1.00776960e-02 1.67961600e-02\n",
            "  2.79936000e-02 4.66560000e-02 7.77600000e-02 1.29600000e-01\n",
            "  2.16000000e-01 3.60000000e-01 6.00000000e-01 1.00000000e+00]\n",
            " [2.28365826e-03 3.42548739e-03 5.13823109e-03 7.70734663e-03\n",
            "  1.15610199e-02 1.73415299e-02 2.60122949e-02 3.90184423e-02\n",
            "  5.85276635e-02 8.77914952e-02 1.31687243e-01 1.97530864e-01\n",
            "  2.96296296e-01 4.44444444e-01 6.66666667e-01 1.00000000e+00]\n",
            " [9.53940729e-03 1.30082827e-02 1.77385673e-02 2.41889554e-02\n",
            "  3.29849391e-02 4.49794625e-02 6.13356306e-02 8.36394963e-02\n",
            "  1.14053859e-01 1.55527989e-01 2.12083621e-01 2.89204938e-01\n",
            "  3.94370370e-01 5.37777778e-01 7.33333333e-01 1.00000000e+00]\n",
            " [3.51843721e-02 4.39804651e-02 5.49755814e-02 6.87194767e-02\n",
            "  8.58993459e-02 1.07374182e-01 1.34217728e-01 1.67772160e-01\n",
            "  2.09715200e-01 2.62144000e-01 3.27680000e-01 4.09600000e-01\n",
            "  5.12000000e-01 6.40000000e-01 8.00000000e-01 1.00000000e+00]\n",
            " [1.16891087e-01 1.34874332e-01 1.55624229e-01 1.79566418e-01\n",
            "  2.07192021e-01 2.39067716e-01 2.75847365e-01 3.18285421e-01\n",
            "  3.67252409e-01 4.23752779e-01 4.88945514e-01 5.64167901e-01\n",
            "  6.50962963e-01 7.51111111e-01 8.66666667e-01 1.00000000e+00]\n",
            " [3.55264366e-01 3.80640393e-01 4.07828992e-01 4.36959634e-01\n",
            "  4.68171037e-01 5.01611825e-01 5.37441241e-01 5.75829901e-01\n",
            "  6.16960609e-01 6.61029224e-01 7.08245597e-01 7.58834568e-01\n",
            "  8.13037037e-01 8.71111111e-01 9.33333333e-01 1.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]]\n",
            "\n",
            "Initial block ranks:\n",
            "[[2 2]\n",
            " [2 2]]\n",
            "\n",
            "Starting iterative refinement...\n",
            "\n",
            "Iteration 1:\n",
            "  Block (0,0): rank=2, rel_error=0.0007, cost~128\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=2, rel_error=0.0069, cost~128\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0188\n",
            "  Total approximate multiplication cost for all blocks: 512\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "Iteration 2:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=1, rel_error=0.1618, cost~64\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=1, rel_error=0.2863, cost~64\n",
            "  Average relative error: 0.1402\n",
            "  Total approximate multiplication cost for all blocks: 256\n",
            "  Updated ranks:\n",
            "[[1 2]\n",
            " [1 2]]\n",
            "--------------------------------------------------\n",
            "Iteration 3:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0450\n",
            "  Total approximate multiplication cost for all blocks: 384\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "Iteration 4:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=1, rel_error=0.1618, cost~64\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=1, rel_error=0.2863, cost~64\n",
            "  Average relative error: 0.1402\n",
            "  Total approximate multiplication cost for all blocks: 256\n",
            "  Updated ranks:\n",
            "[[1 2]\n",
            " [1 2]]\n",
            "--------------------------------------------------\n",
            "Iteration 5:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=2, rel_error=0.0229, cost~128\n",
            "  Block (1,0): rank=1, rel_error=0.0891, cost~64\n",
            "  Block (1,1): rank=2, rel_error=0.0447, cost~128\n",
            "  Average relative error: 0.0450\n",
            "  Total approximate multiplication cost for all blocks: 384\n",
            "  Updated ranks:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Final overall relative reconstruction error: 0.2336335896759094\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper function: Compute truncated SVD reconstruction and relative error.\n",
        "def truncated_svd_reconstruction(block, rank):\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    # Truncate to desired rank (cannot exceed min(block dimensions))\n",
        "    r = min(rank, len(S))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    # Reconstruct approximation\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    # Compute relative Frobenius norm error\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error\n",
        "\n",
        "# Parameters\n",
        "n = 16  # matrix size\n",
        "num_blocks_per_dim = 2  # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "initial_rank = 2\n",
        "error_threshold = 0.1  # 10% relative error allowed initially\n",
        "num_iterations = 5\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "# Using np.vander creates a matrix with columns [x^(n-1), x^(n-2), ..., 1]\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks and initialize a rank matrix\n",
        "ranks = np.full((num_blocks_per_dim, num_blocks_per_dim), initial_rank, dtype=int)\n",
        "\n",
        "# Function to compute approximate multiplication cost for a block using its decomposition\n",
        "def multiplication_cost(b, r):\n",
        "    # Approximate cost: cost for multiplying b x r and r x b matrices (ignoring SVD overhead)\n",
        "    return b * b * r\n",
        "\n",
        "print(\"Initial Structured Matrix (M):\")\n",
        "print(M)\n",
        "print(\"\\nInitial block ranks:\")\n",
        "print(ranks)\n",
        "print(\"\\nStarting iterative refinement...\\n\")\n",
        "\n",
        "# Iterative refinement loop (simulating the RL feedback loop)\n",
        "for it in range(num_iterations):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    new_ranks = ranks.copy()\n",
        "    print(f\"Iteration {it+1}:\")\n",
        "\n",
        "    # Loop over blocks\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            # Extract block from M\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            current_rank = ranks[i, j]\n",
        "            # Get truncated SVD reconstruction and error\n",
        "            _, error = truncated_svd_reconstruction(block, current_rank)\n",
        "            total_error += error\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "\n",
        "            print(f\"  Block ({i},{j}): rank={current_rank}, rel_error={error:.4f}, cost~{cost}\")\n",
        "\n",
        "            # Update rule:\n",
        "            # If error exceeds threshold, use a higher rank next time (less deep decomposition)\n",
        "            # If error is well below threshold, we can try to lower the rank to reduce multiplications.\n",
        "            if error > error_threshold:\n",
        "                new_ranks[i, j] = current_rank + 1  # reduce decomposition (increase rank)\n",
        "            elif error < error_threshold * 0.5 and current_rank > 1:\n",
        "                new_ranks[i, j] = current_rank - 1  # can afford more decomposition (reduce rank)\n",
        "            else:\n",
        "                new_ranks[i, j] = current_rank  # keep as is\n",
        "\n",
        "    ranks = new_ranks\n",
        "    avg_error = total_error / (num_blocks_per_dim * num_blocks_per_dim)\n",
        "    print(f\"  Average relative error: {avg_error:.4f}\")\n",
        "    print(f\"  Total approximate multiplication cost for all blocks: {total_cost}\")\n",
        "    print(\"  Updated ranks:\")\n",
        "    print(ranks)\n",
        "    print(\"-\"*50)\n",
        "\n",
        "# Final output: reconstructed matrix from blocks (for demonstration)\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        r = ranks[i, j]\n",
        "        block_approx, _ = truncated_svd_reconstruction(block, r)\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 2 code - reduce error"
      ],
      "metadata": {
        "id": "iM3yLNk1vVBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def truncated_svd_reconstruction(block, rank, reg=0.0):\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    # Optionally add regularization\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error\n",
        "\n",
        "# Parameters update\n",
        "n = 16\n",
        "num_blocks_per_dim = 2\n",
        "block_size = n // num_blocks_per_dim\n",
        "initial_rank = 3  # increased initial rank\n",
        "error_threshold = 0.1\n",
        "num_iterations = 10  # increased iterations\n",
        "reg_param = 0.01   # regularization parameter for SVD\n",
        "\n",
        "M = np.vander(np.linspace(0, 1, n), N=n, increasing=False)\n",
        "ranks = np.full((num_blocks_per_dim, num_blocks_per_dim), initial_rank, dtype=int)\n",
        "\n",
        "def multiplication_cost(b, r):\n",
        "    return b * b * r\n",
        "\n",
        "print(\"Initial Structured Matrix (M):\")\n",
        "print(M)\n",
        "print(\"\\nInitial block ranks:\")\n",
        "print(ranks)\n",
        "print(\"\\nStarting iterative refinement...\\n\")\n",
        "\n",
        "for it in range(num_iterations):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    new_ranks = ranks.copy()\n",
        "    print(f\"Iteration {it+1}:\")\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            current_rank = ranks[i, j]\n",
        "            _, error = truncated_svd_reconstruction(block, current_rank, reg=reg_param)\n",
        "            total_error += error\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "\n",
        "            print(f\"  Block ({i},{j}): rank={current_rank}, rel_error={error:.4f}, cost~{cost}\")\n",
        "\n",
        "            # Proportional update rule:\n",
        "            if error > error_threshold:\n",
        "                # Increase rank proportional to error excess (round to nearest int)\n",
        "                delta = int(np.ceil((error - error_threshold) / error_threshold))\n",
        "                new_ranks[i, j] = current_rank + delta\n",
        "            elif error < error_threshold * 0.5 and current_rank > 1:\n",
        "                # Decrease rank modestly if error is very low\n",
        "                delta = int(np.ceil((error_threshold * 0.5 - error) / error_threshold))\n",
        "                new_ranks[i, j] = max(1, current_rank - delta)\n",
        "            else:\n",
        "                new_ranks[i, j] = current_rank\n",
        "\n",
        "    ranks = new_ranks\n",
        "    avg_error = total_error / (num_blocks_per_dim * num_blocks_per_dim)\n",
        "    print(f\"  Average relative error: {avg_error:.4f}\")\n",
        "    print(f\"  Total approximate multiplication cost for all blocks: {total_cost}\")\n",
        "    print(\"  Updated ranks:\")\n",
        "    print(ranks)\n",
        "    print(\"-\"*50)\n",
        "\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        r = ranks[i, j]\n",
        "        block_approx, _ = truncated_svd_reconstruction(block, r, reg=reg_param)\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phi_TkGPvZqT",
        "outputId": "32557695-2f1b-408a-cf34-8a932ab91525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Structured Matrix (M):\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            " [2.28365826e-18 3.42548739e-17 5.13823109e-16 7.70734663e-15\n",
            "  1.15610199e-13 1.73415299e-12 2.60122949e-11 3.90184423e-10\n",
            "  5.85276635e-09 8.77914952e-08 1.31687243e-06 1.97530864e-05\n",
            "  2.96296296e-04 4.44444444e-03 6.66666667e-02 1.00000000e+00]\n",
            " [7.48309139e-14 5.61231854e-13 4.20923891e-12 3.15692918e-11\n",
            "  2.36769688e-10 1.77577266e-09 1.33182950e-08 9.98872123e-08\n",
            "  7.49154092e-07 5.61865569e-06 4.21399177e-05 3.16049383e-04\n",
            "  2.37037037e-03 1.77777778e-02 1.33333333e-01 1.00000000e+00]\n",
            " [3.27680000e-11 1.63840000e-10 8.19200000e-10 4.09600000e-09\n",
            "  2.04800000e-08 1.02400000e-07 5.12000000e-07 2.56000000e-06\n",
            "  1.28000000e-05 6.40000000e-05 3.20000000e-04 1.60000000e-03\n",
            "  8.00000000e-03 4.00000000e-02 2.00000000e-01 1.00000000e+00]\n",
            " [2.45205939e-09 9.19522270e-09 3.44820851e-08 1.29307819e-07\n",
            "  4.84904322e-07 1.81839121e-06 6.81896703e-06 2.55711264e-05\n",
            "  9.58917238e-05 3.59593964e-04 1.34847737e-03 5.05679012e-03\n",
            "  1.89629630e-02 7.11111111e-02 2.66666667e-01 1.00000000e+00]\n",
            " [6.96917194e-08 2.09075158e-07 6.27225474e-07 1.88167642e-06\n",
            "  5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
            "  4.57247371e-04 1.37174211e-03 4.11522634e-03 1.23456790e-02\n",
            "  3.70370370e-02 1.11111111e-01 3.33333333e-01 1.00000000e+00]\n",
            " [1.07374182e-06 2.68435456e-06 6.71088640e-06 1.67772160e-05\n",
            "  4.19430400e-05 1.04857600e-04 2.62144000e-04 6.55360000e-04\n",
            "  1.63840000e-03 4.09600000e-03 1.02400000e-02 2.56000000e-02\n",
            "  6.40000000e-02 1.60000000e-01 4.00000000e-01 1.00000000e+00]\n",
            " [1.08418081e-05 2.32324458e-05 4.97838125e-05 1.06679598e-04\n",
            "  2.28599139e-04 4.89855298e-04 1.04968992e-03 2.24933555e-03\n",
            "  4.82000476e-03 1.03285816e-02 2.21326749e-02 4.74271605e-02\n",
            "  1.01629630e-01 2.17777778e-01 4.66666667e-01 1.00000000e+00]\n",
            " [8.03490820e-05 1.50654529e-04 2.82477241e-04 5.29644827e-04\n",
            "  9.93084051e-04 1.86203260e-03 3.49131112e-03 6.54620835e-03\n",
            "  1.22741406e-02 2.30140137e-02 4.31512757e-02 8.09086420e-02\n",
            "  1.51703704e-01 2.84444444e-01 5.33333333e-01 1.00000000e+00]\n",
            " [4.70184985e-04 7.83641641e-04 1.30606940e-03 2.17678234e-03\n",
            "  3.62797056e-03 6.04661760e-03 1.00776960e-02 1.67961600e-02\n",
            "  2.79936000e-02 4.66560000e-02 7.77600000e-02 1.29600000e-01\n",
            "  2.16000000e-01 3.60000000e-01 6.00000000e-01 1.00000000e+00]\n",
            " [2.28365826e-03 3.42548739e-03 5.13823109e-03 7.70734663e-03\n",
            "  1.15610199e-02 1.73415299e-02 2.60122949e-02 3.90184423e-02\n",
            "  5.85276635e-02 8.77914952e-02 1.31687243e-01 1.97530864e-01\n",
            "  2.96296296e-01 4.44444444e-01 6.66666667e-01 1.00000000e+00]\n",
            " [9.53940729e-03 1.30082827e-02 1.77385673e-02 2.41889554e-02\n",
            "  3.29849391e-02 4.49794625e-02 6.13356306e-02 8.36394963e-02\n",
            "  1.14053859e-01 1.55527989e-01 2.12083621e-01 2.89204938e-01\n",
            "  3.94370370e-01 5.37777778e-01 7.33333333e-01 1.00000000e+00]\n",
            " [3.51843721e-02 4.39804651e-02 5.49755814e-02 6.87194767e-02\n",
            "  8.58993459e-02 1.07374182e-01 1.34217728e-01 1.67772160e-01\n",
            "  2.09715200e-01 2.62144000e-01 3.27680000e-01 4.09600000e-01\n",
            "  5.12000000e-01 6.40000000e-01 8.00000000e-01 1.00000000e+00]\n",
            " [1.16891087e-01 1.34874332e-01 1.55624229e-01 1.79566418e-01\n",
            "  2.07192021e-01 2.39067716e-01 2.75847365e-01 3.18285421e-01\n",
            "  3.67252409e-01 4.23752779e-01 4.88945514e-01 5.64167901e-01\n",
            "  6.50962963e-01 7.51111111e-01 8.66666667e-01 1.00000000e+00]\n",
            " [3.55264366e-01 3.80640393e-01 4.07828992e-01 4.36959634e-01\n",
            "  4.68171037e-01 5.01611825e-01 5.37441241e-01 5.75829901e-01\n",
            "  6.16960609e-01 6.61029224e-01 7.08245597e-01 7.58834568e-01\n",
            "  8.13037037e-01 8.71111111e-01 9.33333333e-01 1.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]]\n",
            "\n",
            "Initial block ranks:\n",
            "[[3 3]\n",
            " [3 3]]\n",
            "\n",
            "Starting iterative refinement...\n",
            "\n",
            "Iteration 1:\n",
            "  Block (0,0): rank=3, rel_error=0.0000, cost~192\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=3, rel_error=0.1871, cost~192\n",
            "  Average relative error: 0.0890\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[2 3]\n",
            " [3 4]]\n",
            "--------------------------------------------------\n",
            "Iteration 2:\n",
            "  Block (0,0): rank=2, rel_error=0.0007, cost~128\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=4, rel_error=0.1870, cost~256\n",
            "  Average relative error: 0.0891\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 5]]\n",
            "--------------------------------------------------\n",
            "Iteration 3:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=5, rel_error=0.1870, cost~320\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 6]]\n",
            "--------------------------------------------------\n",
            "Iteration 4:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=6, rel_error=0.1870, cost~384\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 832\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 7]]\n",
            "--------------------------------------------------\n",
            "Iteration 5:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=7, rel_error=0.1870, cost~448\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 896\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 8]]\n",
            "--------------------------------------------------\n",
            "Iteration 6:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=8, rel_error=0.1870, cost~512\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 960\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 9]]\n",
            "--------------------------------------------------\n",
            "Iteration 7:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=9, rel_error=0.1870, cost~576\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1024\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 10]]\n",
            "--------------------------------------------------\n",
            "Iteration 8:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=10, rel_error=0.1870, cost~640\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1088\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 11]]\n",
            "--------------------------------------------------\n",
            "Iteration 9:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=11, rel_error=0.1870, cost~704\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1152\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 12]]\n",
            "--------------------------------------------------\n",
            "Iteration 10:\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=12, rel_error=0.1870, cost~768\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1216\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 13]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Final overall relative reconstruction error: 0.1529940591641265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vesion 3 - enchanced features and reporting"
      ],
      "metadata": {
        "id": "O4Z5LEgAzrkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def truncated_svd_reconstruction(block, rank, reg=0.0):\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    # Optionally apply a simple regularization to S values\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error\n",
        "\n",
        "# Parameters\n",
        "n = 16  # matrix size\n",
        "num_blocks_per_dim = 2  # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "initial_rank = 3  # increased initial rank for better starting accuracy\n",
        "error_threshold = 0.1  # target relative error threshold\n",
        "num_iterations = 10  # increased iterations\n",
        "reg_param = 0.01   # regularization parameter for SVD\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks and initialize a rank matrix\n",
        "ranks = np.full((num_blocks_per_dim, num_blocks_per_dim), initial_rank, dtype=int)\n",
        "\n",
        "def multiplication_cost(b, r):\n",
        "    # Simplified cost model: cost ~ b^2 * r\n",
        "    return b * b * r\n",
        "\n",
        "# Store iteration performance in a list of dictionaries\n",
        "performance_report = []\n",
        "\n",
        "print(\"Initial Structured Matrix (M):\")\n",
        "print(M)\n",
        "print(\"\\nInitial block ranks:\")\n",
        "print(ranks)\n",
        "print(\"\\nStarting iterative refinement with dynamic rank adjustment...\\n\")\n",
        "\n",
        "for it in range(num_iterations):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    new_ranks = ranks.copy()\n",
        "\n",
        "    # Iteration weight increases from 0.1 to 1.0\n",
        "    iteration_weight = (it + 1) / num_iterations\n",
        "\n",
        "    print(f\"Iteration {it+1}: (Iteration weight: {iteration_weight:.2f})\")\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            current_rank = ranks[i, j]\n",
        "            _, error = truncated_svd_reconstruction(block, current_rank, reg=reg_param)\n",
        "            total_error += error\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "\n",
        "            print(f\"  Block ({i},{j}): rank={current_rank}, rel_error={error:.4f}, cost~{cost}\")\n",
        "\n",
        "            # Update rule with iteration_weight:\n",
        "            # In early iterations (low weight), we try to keep cost low (lower ranks)\n",
        "            # In later iterations, if error is high, we allow more increase in rank.\n",
        "            if error > error_threshold:\n",
        "                delta = int(np.ceil((error - error_threshold) / error_threshold * iteration_weight))\n",
        "                new_ranks[i, j] = current_rank + delta\n",
        "            elif error < error_threshold * 0.5 and current_rank > 1:\n",
        "                delta = int(np.ceil((error_threshold * 0.5 - error) / error_threshold * (1 - iteration_weight)))\n",
        "                new_ranks[i, j] = max(1, current_rank - delta)\n",
        "            else:\n",
        "                new_ranks[i, j] = current_rank\n",
        "\n",
        "    ranks = new_ranks\n",
        "    avg_error = total_error / (num_blocks_per_dim * num_blocks_per_dim)\n",
        "    performance_report.append({\n",
        "        'Iteration': it + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "        'Ranks': ranks.copy()\n",
        "    })\n",
        "\n",
        "    print(f\"  Average relative error: {avg_error:.4f}\")\n",
        "    print(f\"  Total approximate multiplication cost for all blocks: {total_cost}\")\n",
        "    print(\"  Updated ranks:\")\n",
        "    print(ranks)\n",
        "    print(\"-\"*50)\n",
        "\n",
        "# Final reconstruction from blocks\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        r = ranks[i, j]\n",
        "        block_approx, _ = truncated_svd_reconstruction(block, r, reg=reg_param)\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n",
        "\n",
        "# Generate a summary report\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(f\"   Ranks per Block:\\n{entry['Ranks']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA2e3uU8zwD7",
        "outputId": "57d87e00-35de-4dd1-c86d-901329e27c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Structured Matrix (M):\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            " [2.28365826e-18 3.42548739e-17 5.13823109e-16 7.70734663e-15\n",
            "  1.15610199e-13 1.73415299e-12 2.60122949e-11 3.90184423e-10\n",
            "  5.85276635e-09 8.77914952e-08 1.31687243e-06 1.97530864e-05\n",
            "  2.96296296e-04 4.44444444e-03 6.66666667e-02 1.00000000e+00]\n",
            " [7.48309139e-14 5.61231854e-13 4.20923891e-12 3.15692918e-11\n",
            "  2.36769688e-10 1.77577266e-09 1.33182950e-08 9.98872123e-08\n",
            "  7.49154092e-07 5.61865569e-06 4.21399177e-05 3.16049383e-04\n",
            "  2.37037037e-03 1.77777778e-02 1.33333333e-01 1.00000000e+00]\n",
            " [3.27680000e-11 1.63840000e-10 8.19200000e-10 4.09600000e-09\n",
            "  2.04800000e-08 1.02400000e-07 5.12000000e-07 2.56000000e-06\n",
            "  1.28000000e-05 6.40000000e-05 3.20000000e-04 1.60000000e-03\n",
            "  8.00000000e-03 4.00000000e-02 2.00000000e-01 1.00000000e+00]\n",
            " [2.45205939e-09 9.19522270e-09 3.44820851e-08 1.29307819e-07\n",
            "  4.84904322e-07 1.81839121e-06 6.81896703e-06 2.55711264e-05\n",
            "  9.58917238e-05 3.59593964e-04 1.34847737e-03 5.05679012e-03\n",
            "  1.89629630e-02 7.11111111e-02 2.66666667e-01 1.00000000e+00]\n",
            " [6.96917194e-08 2.09075158e-07 6.27225474e-07 1.88167642e-06\n",
            "  5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
            "  4.57247371e-04 1.37174211e-03 4.11522634e-03 1.23456790e-02\n",
            "  3.70370370e-02 1.11111111e-01 3.33333333e-01 1.00000000e+00]\n",
            " [1.07374182e-06 2.68435456e-06 6.71088640e-06 1.67772160e-05\n",
            "  4.19430400e-05 1.04857600e-04 2.62144000e-04 6.55360000e-04\n",
            "  1.63840000e-03 4.09600000e-03 1.02400000e-02 2.56000000e-02\n",
            "  6.40000000e-02 1.60000000e-01 4.00000000e-01 1.00000000e+00]\n",
            " [1.08418081e-05 2.32324458e-05 4.97838125e-05 1.06679598e-04\n",
            "  2.28599139e-04 4.89855298e-04 1.04968992e-03 2.24933555e-03\n",
            "  4.82000476e-03 1.03285816e-02 2.21326749e-02 4.74271605e-02\n",
            "  1.01629630e-01 2.17777778e-01 4.66666667e-01 1.00000000e+00]\n",
            " [8.03490820e-05 1.50654529e-04 2.82477241e-04 5.29644827e-04\n",
            "  9.93084051e-04 1.86203260e-03 3.49131112e-03 6.54620835e-03\n",
            "  1.22741406e-02 2.30140137e-02 4.31512757e-02 8.09086420e-02\n",
            "  1.51703704e-01 2.84444444e-01 5.33333333e-01 1.00000000e+00]\n",
            " [4.70184985e-04 7.83641641e-04 1.30606940e-03 2.17678234e-03\n",
            "  3.62797056e-03 6.04661760e-03 1.00776960e-02 1.67961600e-02\n",
            "  2.79936000e-02 4.66560000e-02 7.77600000e-02 1.29600000e-01\n",
            "  2.16000000e-01 3.60000000e-01 6.00000000e-01 1.00000000e+00]\n",
            " [2.28365826e-03 3.42548739e-03 5.13823109e-03 7.70734663e-03\n",
            "  1.15610199e-02 1.73415299e-02 2.60122949e-02 3.90184423e-02\n",
            "  5.85276635e-02 8.77914952e-02 1.31687243e-01 1.97530864e-01\n",
            "  2.96296296e-01 4.44444444e-01 6.66666667e-01 1.00000000e+00]\n",
            " [9.53940729e-03 1.30082827e-02 1.77385673e-02 2.41889554e-02\n",
            "  3.29849391e-02 4.49794625e-02 6.13356306e-02 8.36394963e-02\n",
            "  1.14053859e-01 1.55527989e-01 2.12083621e-01 2.89204938e-01\n",
            "  3.94370370e-01 5.37777778e-01 7.33333333e-01 1.00000000e+00]\n",
            " [3.51843721e-02 4.39804651e-02 5.49755814e-02 6.87194767e-02\n",
            "  8.58993459e-02 1.07374182e-01 1.34217728e-01 1.67772160e-01\n",
            "  2.09715200e-01 2.62144000e-01 3.27680000e-01 4.09600000e-01\n",
            "  5.12000000e-01 6.40000000e-01 8.00000000e-01 1.00000000e+00]\n",
            " [1.16891087e-01 1.34874332e-01 1.55624229e-01 1.79566418e-01\n",
            "  2.07192021e-01 2.39067716e-01 2.75847365e-01 3.18285421e-01\n",
            "  3.67252409e-01 4.23752779e-01 4.88945514e-01 5.64167901e-01\n",
            "  6.50962963e-01 7.51111111e-01 8.66666667e-01 1.00000000e+00]\n",
            " [3.55264366e-01 3.80640393e-01 4.07828992e-01 4.36959634e-01\n",
            "  4.68171037e-01 5.01611825e-01 5.37441241e-01 5.75829901e-01\n",
            "  6.16960609e-01 6.61029224e-01 7.08245597e-01 7.58834568e-01\n",
            "  8.13037037e-01 8.71111111e-01 9.33333333e-01 1.00000000e+00]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]]\n",
            "\n",
            "Initial block ranks:\n",
            "[[3 3]\n",
            " [3 3]]\n",
            "\n",
            "Starting iterative refinement with dynamic rank adjustment...\n",
            "\n",
            "Iteration 1: (Iteration weight: 0.10)\n",
            "  Block (0,0): rank=3, rel_error=0.0000, cost~192\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=3, rel_error=0.1871, cost~192\n",
            "  Average relative error: 0.0890\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[2 3]\n",
            " [3 4]]\n",
            "--------------------------------------------------\n",
            "Iteration 2: (Iteration weight: 0.20)\n",
            "  Block (0,0): rank=2, rel_error=0.0007, cost~128\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=4, rel_error=0.1870, cost~256\n",
            "  Average relative error: 0.0891\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 5]]\n",
            "--------------------------------------------------\n",
            "Iteration 3: (Iteration weight: 0.30)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=5, rel_error=0.1870, cost~320\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 768\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 6]]\n",
            "--------------------------------------------------\n",
            "Iteration 4: (Iteration weight: 0.40)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=6, rel_error=0.1870, cost~384\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 832\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 7]]\n",
            "--------------------------------------------------\n",
            "Iteration 5: (Iteration weight: 0.50)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=7, rel_error=0.1870, cost~448\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 896\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 8]]\n",
            "--------------------------------------------------\n",
            "Iteration 6: (Iteration weight: 0.60)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=8, rel_error=0.1870, cost~512\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 960\n",
            "  Updated ranks:\n",
            "[[1 3]\n",
            " [3 9]]\n",
            "--------------------------------------------------\n",
            "Iteration 7: (Iteration weight: 0.70)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=9, rel_error=0.1870, cost~576\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1024\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 10]]\n",
            "--------------------------------------------------\n",
            "Iteration 8: (Iteration weight: 0.80)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=10, rel_error=0.1870, cost~640\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1088\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 11]]\n",
            "--------------------------------------------------\n",
            "Iteration 9: (Iteration weight: 0.90)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=11, rel_error=0.1870, cost~704\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1152\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 12]]\n",
            "--------------------------------------------------\n",
            "Iteration 10: (Iteration weight: 1.00)\n",
            "  Block (0,0): rank=1, rel_error=0.0234, cost~64\n",
            "  Block (0,1): rank=3, rel_error=0.0774, cost~192\n",
            "  Block (1,0): rank=3, rel_error=0.0914, cost~192\n",
            "  Block (1,1): rank=12, rel_error=0.1870, cost~768\n",
            "  Average relative error: 0.0948\n",
            "  Total approximate multiplication cost for all blocks: 1216\n",
            "  Updated ranks:\n",
            "[[ 1  3]\n",
            " [ 3 13]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Final overall relative reconstruction error: 0.1529940591641265\n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.0890\n",
            "   Total Approximate Multiplication Cost: 768\n",
            "   Ranks per Block:\n",
            "[[2 3]\n",
            " [3 4]]\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.0891\n",
            "   Total Approximate Multiplication Cost: 768\n",
            "   Ranks per Block:\n",
            "[[1 3]\n",
            " [3 5]]\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 768\n",
            "   Ranks per Block:\n",
            "[[1 3]\n",
            " [3 6]]\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 832\n",
            "   Ranks per Block:\n",
            "[[1 3]\n",
            " [3 7]]\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 896\n",
            "   Ranks per Block:\n",
            "[[1 3]\n",
            " [3 8]]\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 960\n",
            "   Ranks per Block:\n",
            "[[1 3]\n",
            " [3 9]]\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 1024\n",
            "   Ranks per Block:\n",
            "[[ 1  3]\n",
            " [ 3 10]]\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 1088\n",
            "   Ranks per Block:\n",
            "[[ 1  3]\n",
            " [ 3 11]]\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 1152\n",
            "   Ranks per Block:\n",
            "[[ 1  3]\n",
            " [ 3 12]]\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.0948\n",
            "   Total Approximate Multiplication Cost: 1216\n",
            "   Ranks per Block:\n",
            "[[ 1  3]\n",
            " [ 3 13]]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 5- major revision to ensure error is reduced in each iteration"
      ],
      "metadata": {
        "id": "cdSRlUslM3Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assemble_block(U_list, S_list, Vt_list, shape):\n",
        "    \"\"\"\n",
        "    Reconstruct a block from lists of rank-1 SVD components.\n",
        "    If no components exist, return a zero matrix of the given shape.\n",
        "    \"\"\"\n",
        "    if len(U_list) == 0:\n",
        "        return np.zeros(shape)\n",
        "    block_approx = np.zeros(shape)\n",
        "    for U_i, S_i, Vt_i in zip(U_list, S_list, Vt_list):\n",
        "        # Ensure Vt_i is a 1D array.\n",
        "        Vt_i = np.array(Vt_i).flatten()\n",
        "        block_approx += np.outer(U_i, Vt_i) * S_i\n",
        "    return block_approx\n",
        "\n",
        "def compute_truncated_svd(block, rank, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute a truncated SVD approximation of the block.\n",
        "    Applies simple regularization to singular values if reg > 0.\n",
        "    Returns the approximation, relative error, and the truncated SVD factors.\n",
        "    \"\"\"\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    # Apply regularization if needed\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error, U_r, S_r, Vt_r\n",
        "\n",
        "def compute_residual(block, U_list, S_list, Vt_list):\n",
        "    \"\"\"\n",
        "    Compute the residual between the block and its current reconstruction.\n",
        "    \"\"\"\n",
        "    block_approx = assemble_block(U_list, S_list, Vt_list, block.shape)\n",
        "    return block - block_approx\n",
        "\n",
        "def multiplication_cost(b, total_rank):\n",
        "    # Estimated cost: cost ~ (b^2 * total_rank)\n",
        "    return b * b * total_rank\n",
        "\n",
        "# Parameters\n",
        "n = 16  # overall matrix size\n",
        "num_blocks_per_dim = 2  # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "max_outer_iters = 10  # maximum number of outer iterations (each may add one rank)\n",
        "inner_iters = 5       # inner iterations per rank level for refinement\n",
        "residual_threshold_ratio = 0.05  # threshold: if top singular value < 5% of block norm, stop adding rank\n",
        "reg_param = 0.01   # regularization parameter for SVD\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks.\n",
        "# For each block, we store a dictionary with lists of rank-1 components: U_list, S_list, Vt_list.\n",
        "components = [[{'U_list': [], 'S_list': [], 'Vt_list': []} for _ in range(num_blocks_per_dim)] for _ in range(num_blocks_per_dim)]\n",
        "\n",
        "# Initialize each block with a rank-1 approximation (using the top singular component).\n",
        "initial_report = {}\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "        # Use only the top singular component.\n",
        "        components[i][j]['U_list'] = [U[:, 0]]\n",
        "        components[i][j]['S_list'] = [S[0]]\n",
        "        components[i][j]['Vt_list'] = [Vt[0, :]]\n",
        "        _, err, _, _, _ = compute_truncated_svd(block, 1, reg=reg_param)\n",
        "        initial_report[f'Block ({i},{j})'] = err\n",
        "\n",
        "print(\"Initial reconstruction errors per block (rank=1):\")\n",
        "for key, err in initial_report.items():\n",
        "    print(f\"{key}: {err:.4f}\")\n",
        "\n",
        "# Store performance data\n",
        "performance_report = []\n",
        "\n",
        "# Outer loop: each iteration may add one new rank-1 component per block if warranted.\n",
        "for outer in range(max_outer_iters):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    # For each block, refine and decide on adding a new component.\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            comp = components[i][j]\n",
        "            current_rank = len(comp['U_list'])\n",
        "\n",
        "            # Inner loop: refine the current components via a simple ALS-like update.\n",
        "            for inner in range(inner_iters):\n",
        "                # For simplicity, recompute a full SVD on the block and update components\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = U_ref[:, k]\n",
        "                    comp['S_list'][k] = S_ref[k]\n",
        "                    comp['Vt_list'][k] = Vt_ref[k, :]\n",
        "\n",
        "            # Compute the residual after inner loop refinement.\n",
        "            residual = compute_residual(block, comp['U_list'], comp['S_list'], comp['Vt_list'])\n",
        "            residual_norm = np.linalg.norm(residual, 'fro')\n",
        "            block_norm = np.linalg.norm(block, 'fro')\n",
        "            rel_residual = residual_norm / block_norm\n",
        "\n",
        "            # Compute top singular value of the residual.\n",
        "            if np.allclose(residual, 0):\n",
        "                top_singular = 0\n",
        "            else:\n",
        "                top_singular = np.linalg.svd(residual, compute_uv=False)[0]\n",
        "\n",
        "            # Decision: add a new rank-1 component if the dominant residual is significant.\n",
        "            if top_singular / block_norm > residual_threshold_ratio:\n",
        "                U_r, S_r, Vt_r = np.linalg.svd(residual, full_matrices=False)\n",
        "                comp['U_list'].append(U_r[:, 0])\n",
        "                comp['S_list'].append(S_r[0])\n",
        "                comp['Vt_list'].append(Vt_r[0, :])\n",
        "                current_rank += 1\n",
        "\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "            total_error += rel_residual\n",
        "\n",
        "    avg_error = total_error / (num_blocks_per_dim**2)\n",
        "    performance_report.append({\n",
        "        'Iteration': outer + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "    })\n",
        "    print(f\"Outer Iteration {outer+1}: Average Relative Error = {avg_error:.4f}, Total Cost ~ {total_cost}\")\n",
        "\n",
        "# Final reconstruction from blocks.\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        comp = components[i][j]\n",
        "        block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n",
        "\n",
        "# Generate a summary report.\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgPKeGSMONRA",
        "outputId": "3f72518b-2141-412f-c10f-248f431cecf7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reconstruction errors per block (rank=1):\n",
            "Block (0,0): 0.0234\n",
            "Block (0,1): 0.1794\n",
            "Block (1,0): 0.1276\n",
            "Block (1,1): 0.3419\n",
            "Outer Iteration 1: Average Relative Error = 0.1402, Total Cost ~ 448\n",
            "Outer Iteration 2: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 3: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 4: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 5: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 6: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 7: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 8: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 9: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 10: Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "\n",
            "Final overall relative reconstruction error: 0.035742282723656905\n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 6 - refinements to improve the performance for each iteration"
      ],
      "metadata": {
        "id": "uP-JoJvxQJAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assemble_block(U_list, S_list, Vt_list, shape):\n",
        "    \"\"\"\n",
        "    Reconstruct a block from lists of rank-1 SVD components.\n",
        "    If no components exist, return a zero matrix of the given shape.\n",
        "    \"\"\"\n",
        "    if len(U_list) == 0:\n",
        "        return np.zeros(shape)\n",
        "    block_approx = np.zeros(shape)\n",
        "    for U_i, S_i, Vt_i in zip(U_list, S_list, Vt_list):\n",
        "        Vt_i = np.array(Vt_i).flatten()\n",
        "        block_approx += np.outer(U_i, Vt_i) * S_i\n",
        "    return block_approx\n",
        "\n",
        "def compute_truncated_svd(block, rank, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute a truncated SVD approximation of the block.\n",
        "    Applies simple regularization to singular values if reg > 0.\n",
        "    Returns the approximation, relative error, and the truncated SVD factors.\n",
        "    \"\"\"\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error, U_r, S_r, Vt_r\n",
        "\n",
        "def compute_residual(block, U_list, S_list, Vt_list):\n",
        "    block_approx = assemble_block(U_list, S_list, Vt_list, block.shape)\n",
        "    return block - block_approx\n",
        "\n",
        "def multiplication_cost(b, total_rank):\n",
        "    # Estimated cost: cost ~ (b^2 * total_rank)\n",
        "    return b * b * total_rank\n",
        "\n",
        "# Parameters\n",
        "n = 16                              # overall matrix size\n",
        "num_blocks_per_dim = 2              # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "max_outer_iters = 10                # maximum number of outer iterations (each may add one rank)\n",
        "inner_iters = 5                     # inner iterations per rank level for refinement\n",
        "extra_inner_iters = 2               # additional refinement after potential rank addition\n",
        "initial_threshold = 0.05            # initial residual threshold ratio\n",
        "decay_factor = 0.95                 # dynamic threshold decays each iteration\n",
        "reg_param = 0.01                    # regularization parameter for SVD\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks.\n",
        "components = [[{'U_list': [], 'S_list': [], 'Vt_list': []} for _ in range(num_blocks_per_dim)]\n",
        "              for _ in range(num_blocks_per_dim)]\n",
        "\n",
        "# Initialize each block with a rank-1 approximation.\n",
        "initial_report = {}\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "        components[i][j]['U_list'] = [U[:, 0]]\n",
        "        components[i][j]['S_list'] = [S[0]]\n",
        "        components[i][j]['Vt_list'] = [Vt[0, :]]\n",
        "        _, err, _, _, _ = compute_truncated_svd(block, 1, reg=reg_param)\n",
        "        initial_report[f'Block ({i},{j})'] = err\n",
        "\n",
        "print(\"Initial reconstruction errors per block (rank=1):\")\n",
        "for key, err in initial_report.items():\n",
        "    print(f\"{key}: {err:.4f}\")\n",
        "\n",
        "performance_report = []\n",
        "\n",
        "# Outer loop: each iteration may add a new rank-1 component if warranted.\n",
        "for outer in range(max_outer_iters):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    # Compute dynamic threshold for this outer iteration.\n",
        "    dynamic_threshold = initial_threshold * (decay_factor ** outer)\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            comp = components[i][j]\n",
        "            current_rank = len(comp['U_list'])\n",
        "\n",
        "            # Inner loop: refine the current components.\n",
        "            for inner in range(inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = U_ref[:, k]\n",
        "                    comp['S_list'][k] = S_ref[k]\n",
        "                    comp['Vt_list'][k] = Vt_ref[k, :]\n",
        "\n",
        "            # Compute residual and its norm.\n",
        "            residual = compute_residual(block, comp['U_list'], comp['S_list'], comp['Vt_list'])\n",
        "            residual_norm = np.linalg.norm(residual, 'fro')\n",
        "            block_norm = np.linalg.norm(block, 'fro')\n",
        "            rel_residual = residual_norm / block_norm\n",
        "\n",
        "            # Compute top singular value of the residual.\n",
        "            if np.allclose(residual, 0):\n",
        "                top_singular = 0\n",
        "            else:\n",
        "                top_singular = np.linalg.svd(residual, compute_uv=False)[0]\n",
        "\n",
        "            # Decision: add new component if potential gain exceeds dynamic threshold.\n",
        "            if top_singular / block_norm > dynamic_threshold:\n",
        "                U_r, S_r, Vt_r = np.linalg.svd(residual, full_matrices=False)\n",
        "                comp['U_list'].append(U_r[:, 0])\n",
        "                comp['S_list'].append(S_r[0])\n",
        "                comp['Vt_list'].append(Vt_r[0, :])\n",
        "                current_rank += 1\n",
        "\n",
        "            # Additional inner-loop refinement after rank addition.\n",
        "            for extra in range(extra_inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = U_ref[:, k]\n",
        "                    comp['S_list'][k] = S_ref[k]\n",
        "                    comp['Vt_list'][k] = Vt_ref[k, :]\n",
        "\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "            total_error += rel_residual\n",
        "\n",
        "    avg_error = total_error / (num_blocks_per_dim ** 2)\n",
        "    performance_report.append({\n",
        "        'Iteration': outer + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "    })\n",
        "    print(f\"Outer Iteration {outer+1}: Dynamic Threshold = {dynamic_threshold:.4f}, Average Relative Error = {avg_error:.4f}, Total Cost ~ {total_cost}\")\n",
        "\n",
        "# Final reconstruction from blocks.\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        comp = components[i][j]\n",
        "        block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n",
        "\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx1oCj8jQVBX",
        "outputId": "fbad4ec5-cfed-4958-8263-cc2ac820376e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reconstruction errors per block (rank=1):\n",
            "Block (0,0): 0.0234\n",
            "Block (0,1): 0.1794\n",
            "Block (1,0): 0.1276\n",
            "Block (1,1): 0.3419\n",
            "Outer Iteration 1: Dynamic Threshold = 0.0500, Average Relative Error = 0.1402, Total Cost ~ 448\n",
            "Outer Iteration 2: Dynamic Threshold = 0.0475, Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 3: Dynamic Threshold = 0.0451, Average Relative Error = 0.0245, Total Cost ~ 448\n",
            "Outer Iteration 4: Dynamic Threshold = 0.0429, Average Relative Error = 0.0245, Total Cost ~ 512\n",
            "Outer Iteration 5: Dynamic Threshold = 0.0407, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "Outer Iteration 6: Dynamic Threshold = 0.0387, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "Outer Iteration 7: Dynamic Threshold = 0.0368, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "Outer Iteration 8: Dynamic Threshold = 0.0349, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "Outer Iteration 9: Dynamic Threshold = 0.0332, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "Outer Iteration 10: Dynamic Threshold = 0.0315, Average Relative Error = 0.0144, Total Cost ~ 512\n",
            "\n",
            "Final overall relative reconstruction error: 0.011104243087954815\n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 448\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.0245\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.0144\n",
            "   Total Approximate Multiplication Cost: 512\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 7 - 5 significant changes to the code to further optomize the output"
      ],
      "metadata": {
        "id": "Y-YojLl0TTie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assemble_block(U_list, S_list, Vt_list, shape):\n",
        "    \"\"\"\n",
        "    Reconstruct a block from lists of rank-1 SVD components.\n",
        "    If no components exist, return a zero matrix of the given shape.\n",
        "    \"\"\"\n",
        "    if len(U_list) == 0:\n",
        "        return np.zeros(shape)\n",
        "    block_approx = np.zeros(shape)\n",
        "    for U_i, S_i, Vt_i in zip(U_list, S_list, Vt_list):\n",
        "        Vt_i = np.array(Vt_i).flatten()\n",
        "        block_approx += np.outer(U_i, Vt_i) * S_i\n",
        "    return block_approx\n",
        "\n",
        "def compute_truncated_svd(block, rank, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute a truncated SVD approximation of the block.\n",
        "    Applies simple regularization to singular values if reg > 0.\n",
        "    Returns the approximation, relative error, and the truncated SVD factors.\n",
        "    \"\"\"\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error, U_r, S_r, Vt_r\n",
        "\n",
        "def compute_residual(block, U_list, S_list, Vt_list):\n",
        "    block_approx = assemble_block(U_list, S_list, Vt_list, block.shape)\n",
        "    return block - block_approx\n",
        "\n",
        "def multiplication_cost(b, total_rank):\n",
        "    # Estimated cost: cost ~ (b^2 * total_rank)\n",
        "    return b * b * total_rank\n",
        "\n",
        "# Parameters\n",
        "n = 16                              # overall matrix size\n",
        "num_blocks_per_dim = 2              # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "max_outer_iters = 10                # maximum outer iterations (each may add one rank)\n",
        "inner_iters = 5                     # inner iterations per rank level for refinement\n",
        "extra_inner_iters = 3               # extra refinement iterations after rank addition\n",
        "global_initial_threshold = 0.05     # global minimal threshold\n",
        "decay_factor = 0.95                 # decay factor for dynamic threshold per outer iteration\n",
        "reg_param = 0.01                    # regularization parameter for SVD\n",
        "alpha = 0.5                         # weight for blending old and new components\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks.\n",
        "# For each block, we store a dictionary with lists of rank-1 components and also record its baseline threshold.\n",
        "components = [[{'U_list': [], 'S_list': [], 'Vt_list': [], 'baseline_threshold': None}\n",
        "               for _ in range(num_blocks_per_dim)] for _ in range(num_blocks_per_dim)]\n",
        "\n",
        "# Initialize each block with a rank-1 approximation.\n",
        "initial_report = {}\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "        components[i][j]['U_list'] = [U[:, 0]]\n",
        "        components[i][j]['S_list'] = [S[0]]\n",
        "        components[i][j]['Vt_list'] = [Vt[0, :]]\n",
        "        _, err, _, _, _ = compute_truncated_svd(block, 1, reg=reg_param)\n",
        "        initial_report[f'Block ({i},{j})'] = err\n",
        "        # Set baseline threshold as the initial error ratio.\n",
        "        components[i][j]['baseline_threshold'] = err\n",
        "\n",
        "print(\"Initial reconstruction errors per block (rank=1):\")\n",
        "for key, err in initial_report.items():\n",
        "    print(f\"{key}: {err:.4f}\")\n",
        "\n",
        "performance_report = []\n",
        "\n",
        "# Outer loop: each iteration may add a new rank-1 component if warranted.\n",
        "for outer in range(max_outer_iters):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            comp = components[i][j]\n",
        "            current_rank = len(comp['U_list'])\n",
        "\n",
        "            # Inner loop: refine the current components with weighted updates.\n",
        "            for inner in range(inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    # Blend old and new components.\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            # Compute residual after inner loop refinement.\n",
        "            residual = compute_residual(block, comp['U_list'], comp['S_list'], comp['Vt_list'])\n",
        "            residual_norm = np.linalg.norm(residual, 'fro')\n",
        "            block_norm = np.linalg.norm(block, 'fro')\n",
        "            rel_residual = residual_norm / block_norm\n",
        "\n",
        "            # Enhanced residual metric: use cumulative energy of top 2 singular values.\n",
        "            sing_vals = np.linalg.svd(residual, compute_uv=False)\n",
        "            if np.sum(sing_vals) == 0:\n",
        "                energy_gap = 0\n",
        "            else:\n",
        "                if len(sing_vals) > 1:\n",
        "                    energy_ratio = np.sum(sing_vals[:2]) / np.sum(sing_vals)\n",
        "                else:\n",
        "                    energy_ratio = sing_vals[0] / np.sum(sing_vals)\n",
        "                energy_gap = 1 - energy_ratio\n",
        "\n",
        "            # Per-block dynamic threshold: use the block's baseline or global minimum, whichever is higher.\n",
        "            block_baseline = comp['baseline_threshold']\n",
        "            dynamic_threshold = max(global_initial_threshold, block_baseline) * (decay_factor ** outer)\n",
        "\n",
        "            # Decide on adding a new component based on enhanced metric.\n",
        "            if energy_gap > dynamic_threshold:\n",
        "                # Add new component using SVD of the residual.\n",
        "                U_r, S_r, Vt_r = np.linalg.svd(residual, full_matrices=False)\n",
        "                comp['U_list'].append(U_r[:, 0])\n",
        "                comp['S_list'].append(S_r[0])\n",
        "                comp['Vt_list'].append(Vt_r[0, :])\n",
        "                current_rank += 1\n",
        "\n",
        "            # Extra inner-loop refinement after potential rank addition.\n",
        "            for extra in range(extra_inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "            total_error += rel_residual\n",
        "\n",
        "    avg_error = total_error / (num_blocks_per_dim ** 2)\n",
        "    performance_report.append({\n",
        "        'Iteration': outer + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "    })\n",
        "    print(f\"Outer Iteration {outer+1}:\")\n",
        "    print(f\"   Dynamic Threshold (avg across blocks): {np.mean([max(global_initial_threshold, components[i][j]['baseline_threshold'])*(decay_factor**outer) for i in range(num_blocks_per_dim) for j in range(num_blocks_per_dim)]):.4f}\")\n",
        "    print(f\"   Average Relative Error = {avg_error:.4f}, Total Cost ~ {total_cost}\")\n",
        "\n",
        "# Global reoptimization: reassemble the full approximation and perform a global SVD update.\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        comp = components[i][j]\n",
        "        block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "# Global SVD refinement.\n",
        "U_global, S_global, Vt_global = np.linalg.svd(M, full_matrices=False)\n",
        "M_global_approx = U_global @ np.diag(S_global) @ Vt_global\n",
        "# Update each block's approximation from the global approximation.\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = \\\n",
        "            M_global_approx[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error:\", final_error)\n",
        "\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JID_k_oCTfXX",
        "outputId": "91476dc1-34d9-4a0a-ca04-79a9a040f748"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reconstruction errors per block (rank=1):\n",
            "Block (0,0): 0.0234\n",
            "Block (0,1): 0.1794\n",
            "Block (1,0): 0.1276\n",
            "Block (1,1): 0.3419\n",
            "Outer Iteration 1:\n",
            "   Dynamic Threshold (avg across blocks): 0.1747\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 2:\n",
            "   Dynamic Threshold (avg across blocks): 0.1660\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 3:\n",
            "   Dynamic Threshold (avg across blocks): 0.1577\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 4:\n",
            "   Dynamic Threshold (avg across blocks): 0.1498\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 5:\n",
            "   Dynamic Threshold (avg across blocks): 0.1423\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 6:\n",
            "   Dynamic Threshold (avg across blocks): 0.1352\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 7:\n",
            "   Dynamic Threshold (avg across blocks): 0.1284\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 8:\n",
            "   Dynamic Threshold (avg across blocks): 0.1220\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 9:\n",
            "   Dynamic Threshold (avg across blocks): 0.1159\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 10:\n",
            "   Dynamic Threshold (avg across blocks): 0.1101\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "\n",
            "Final overall relative reconstruction error: 1.134140680586365e-15\n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparision of the iterative deconstruction method vs standard matrix mathematics method"
      ],
      "metadata": {
        "id": "nrbO343MYt2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- (Assume the previous iterative decomposition code has run and produced final_error and cost) ----------\n",
        "\n",
        "# For comparison, we set:\n",
        "n = 16\n",
        "cost_std = n ** 3  # Standard multiplication cost: 16^3 = 4096 multiplications\n",
        "error_std = 1e-15  # Assume near-perfect accuracy for standard multiplication\n",
        "efficiency_std = (1 - error_std) / cost_std\n",
        "\n",
        "# From our iterative method:\n",
        "# cost_decomp is the total cost from our performance report.\n",
        "# From the output, cost_decomp is 256 and final_error is ~1.134e-15.\n",
        "cost_decomp = 256\n",
        "error_decomp = final_error  # final_error computed earlier from our algorithm\n",
        "efficiency_decomp = (1 - error_decomp) / cost_decomp\n",
        "\n",
        "# Create a comparison table.\n",
        "data = {\n",
        "    \"Method\": [\"Standard Multiplication\", \"Decomposition-Based Multiplication\"],\n",
        "    \"Cost (Multiplications)\": [cost_std, cost_decomp],\n",
        "    \"Relative Error\": [error_std, error_decomp],\n",
        "    \"Efficiency Metric\": [efficiency_std, efficiency_decomp]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDUK1cCMY_Nr",
        "outputId": "939f5178-4ad9-4db6-bff7-b4d046ef9e5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               Method  Cost (Multiplications)  Relative Error  \\\n",
            "0             Standard Multiplication                    4096    1.000000e-15   \n",
            "1  Decomposition-Based Multiplication                     256    1.134141e-15   \n",
            "\n",
            "   Efficiency Metric  \n",
            "0           0.000244  \n",
            "1           0.003906  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 8 - comprehensive comparision of the decomposition-based multiplication method versus standard matrix multiplication"
      ],
      "metadata": {
        "id": "A1oESgW4as0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def assemble_block(U_list, S_list, Vt_list, shape):\n",
        "    \"\"\"\n",
        "    Reconstruct a block from lists of rank-1 SVD components.\n",
        "    If no components exist, return a zero matrix of the given shape.\n",
        "    \"\"\"\n",
        "    if len(U_list) == 0:\n",
        "        return np.zeros(shape)\n",
        "    block_approx = np.zeros(shape)\n",
        "    for U_i, S_i, Vt_i in zip(U_list, S_list, Vt_list):\n",
        "        Vt_i = np.array(Vt_i).flatten()\n",
        "        block_approx += np.outer(U_i, Vt_i) * S_i\n",
        "    return block_approx\n",
        "\n",
        "def compute_truncated_svd(block, rank, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute a truncated SVD approximation of the block.\n",
        "    Applies simple regularization to singular values if reg > 0.\n",
        "    Returns the approximation, relative error, and the truncated SVD factors.\n",
        "    \"\"\"\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error, U_r, S_r, Vt_r\n",
        "\n",
        "def compute_residual(block, U_list, S_list, Vt_list):\n",
        "    block_approx = assemble_block(U_list, S_list, Vt_list, block.shape)\n",
        "    return block - block_approx\n",
        "\n",
        "def multiplication_cost(b, total_rank):\n",
        "    # Estimated cost: cost ~ (b^2 * total_rank)\n",
        "    return b * b * total_rank\n",
        "\n",
        "# ------------------ Decomposition-Based Multiplication Method ------------------\n",
        "\n",
        "# Parameters\n",
        "n = 16                              # overall matrix size\n",
        "num_blocks_per_dim = 2              # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "max_outer_iters = 10                # maximum outer iterations (each may add one rank)\n",
        "inner_iters = 5                     # inner iterations per rank level for refinement\n",
        "extra_inner_iters = 3               # extra refinement iterations after rank addition\n",
        "global_initial_threshold = 0.05     # global minimal threshold\n",
        "decay_factor = 0.95                 # decay factor for dynamic threshold per outer iteration\n",
        "reg_param = 0.01                    # regularization parameter for SVD\n",
        "alpha = 0.5                         # weight for blending old and new components\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks.\n",
        "# For each block, we store a dictionary with lists of rank-1 components and also record its baseline threshold.\n",
        "components = [[{'U_list': [], 'S_list': [], 'Vt_list': [], 'baseline_threshold': None}\n",
        "               for _ in range(num_blocks_per_dim)] for _ in range(num_blocks_per_dim)]\n",
        "\n",
        "# Initialize each block with a rank-1 approximation.\n",
        "initial_report = {}\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "        components[i][j]['U_list'] = [U[:, 0]]\n",
        "        components[i][j]['S_list'] = [S[0]]\n",
        "        components[i][j]['Vt_list'] = [Vt[0, :]]\n",
        "        _, err, _, _, _ = compute_truncated_svd(block, 1, reg=reg_param)\n",
        "        initial_report[f'Block ({i},{j})'] = err\n",
        "        # Set baseline threshold as the initial error.\n",
        "        components[i][j]['baseline_threshold'] = err\n",
        "\n",
        "print(\"Initial reconstruction errors per block (rank=1):\")\n",
        "for key, err in initial_report.items():\n",
        "    print(f\"{key}: {err:.4f}\")\n",
        "\n",
        "performance_report = []\n",
        "\n",
        "# Outer loop: each iteration may add a new rank-1 component if warranted.\n",
        "for outer in range(max_outer_iters):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            comp = components[i][j]\n",
        "            current_rank = len(comp['U_list'])\n",
        "\n",
        "            # Inner loop: refine the current components with weighted updates.\n",
        "            for inner in range(inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    # Blend the old component with the new one.\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            # Compute residual after inner-loop refinement.\n",
        "            residual = compute_residual(block, comp['U_list'], comp['S_list'], comp['Vt_list'])\n",
        "            residual_norm = np.linalg.norm(residual, 'fro')\n",
        "            block_norm = np.linalg.norm(block, 'fro')\n",
        "            rel_residual = residual_norm / block_norm\n",
        "\n",
        "            # Enhanced residual metric: use cumulative energy of top 2 singular values.\n",
        "            sing_vals = np.linalg.svd(residual, compute_uv=False)\n",
        "            if np.sum(sing_vals) == 0:\n",
        "                energy_gap = 0\n",
        "            else:\n",
        "                if len(sing_vals) > 1:\n",
        "                    energy_ratio = np.sum(sing_vals[:2]) / np.sum(sing_vals)\n",
        "                else:\n",
        "                    energy_ratio = sing_vals[0] / np.sum(sing_vals)\n",
        "                energy_gap = 1 - energy_ratio\n",
        "\n",
        "            # Per-block dynamic threshold: use the block's baseline or global minimum, whichever is higher.\n",
        "            block_baseline = comp['baseline_threshold']\n",
        "            dynamic_threshold = max(global_initial_threshold, block_baseline) * (decay_factor ** outer)\n",
        "\n",
        "            # Decide on adding a new component based on enhanced metric.\n",
        "            if energy_gap > dynamic_threshold:\n",
        "                U_r, S_r, Vt_r = np.linalg.svd(residual, full_matrices=False)\n",
        "                comp['U_list'].append(U_r[:, 0])\n",
        "                comp['S_list'].append(S_r[0])\n",
        "                comp['Vt_list'].append(Vt_r[0, :])\n",
        "                current_rank += 1\n",
        "\n",
        "            # Extra inner-loop refinement after rank addition.\n",
        "            for extra in range(extra_inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "            total_error += rel_residual\n",
        "\n",
        "    avg_error = total_error / (num_blocks_per_dim ** 2)\n",
        "    performance_report.append({\n",
        "        'Iteration': outer + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "    })\n",
        "    avg_dynamic_threshold = np.mean([max(global_initial_threshold, components[i][j]['baseline_threshold'])*(decay_factor**outer)\n",
        "                                       for i in range(num_blocks_per_dim) for j in range(num_blocks_per_dim)])\n",
        "    print(f\"Outer Iteration {outer+1}:\")\n",
        "    print(f\"   Dynamic Threshold (avg across blocks): {avg_dynamic_threshold:.4f}\")\n",
        "    print(f\"   Average Relative Error = {avg_error:.4f}, Total Cost ~ {total_cost}\")\n",
        "\n",
        "# Global reoptimization: reassemble full approximation and perform a global SVD update.\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        comp = components[i][j]\n",
        "        block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "# Global SVD refinement.\n",
        "U_global, S_global, Vt_global = np.linalg.svd(M, full_matrices=False)\n",
        "M_global_approx = U_global @ np.diag(S_global) @ Vt_global\n",
        "# Replace block approximations with corresponding blocks from global approximation.\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = \\\n",
        "            M_global_approx[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error (Decomposition-Based):\", final_error)\n",
        "\n",
        "# ------------------ Standard Matrix Multiplication Comparison ------------------\n",
        "\n",
        "# Standard matrix multiplication: cost is defined as n^3 multiplications.\n",
        "cost_std = n ** 3  # For n=16, cost = 4096\n",
        "error_std = 1e-15  # Assume near-perfect accuracy (machine precision)\n",
        "efficiency_std = (1 - error_std) / cost_std\n",
        "\n",
        "# For the decomposition method:\n",
        "# Here, we take the total cost from the last outer iteration.\n",
        "if performance_report:\n",
        "    cost_decomp = performance_report[-1]['Total Cost']\n",
        "else:\n",
        "    cost_decomp = None\n",
        "error_decomp = final_error\n",
        "efficiency_decomp = (1 - error_decomp) / cost_decomp if cost_decomp else None\n",
        "\n",
        "# Create a comparison table.\n",
        "data = {\n",
        "    \"Method\": [\"Standard Multiplication\", \"Decomposition-Based Multiplication\"],\n",
        "    \"Cost (Multiplications)\": [cost_std, cost_decomp],\n",
        "    \"Relative Error\": [error_std, error_decomp],\n",
        "    \"Efficiency Metric\": [efficiency_std, efficiency_decomp]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\nComparison of Matrix Multiplication Methods:\")\n",
        "print(df)\n",
        "\n",
        "# ------------------ Performance Report Summary ------------------\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFTX-gdba5lA",
        "outputId": "1c829a73-a60a-4501-a213-2747b255c278"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reconstruction errors per block (rank=1):\n",
            "Block (0,0): 0.0234\n",
            "Block (0,1): 0.1794\n",
            "Block (1,0): 0.1276\n",
            "Block (1,1): 0.3419\n",
            "Outer Iteration 1:\n",
            "   Dynamic Threshold (avg across blocks): 0.1747\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 2:\n",
            "   Dynamic Threshold (avg across blocks): 0.1660\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 3:\n",
            "   Dynamic Threshold (avg across blocks): 0.1577\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 4:\n",
            "   Dynamic Threshold (avg across blocks): 0.1498\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 5:\n",
            "   Dynamic Threshold (avg across blocks): 0.1423\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 6:\n",
            "   Dynamic Threshold (avg across blocks): 0.1352\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 7:\n",
            "   Dynamic Threshold (avg across blocks): 0.1284\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 8:\n",
            "   Dynamic Threshold (avg across blocks): 0.1220\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 9:\n",
            "   Dynamic Threshold (avg across blocks): 0.1159\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "Outer Iteration 10:\n",
            "   Dynamic Threshold (avg across blocks): 0.1101\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 256\n",
            "\n",
            "Final overall relative reconstruction error (Decomposition-Based): 1.134140680586365e-15\n",
            "\n",
            "Comparison of Matrix Multiplication Methods:\n",
            "                               Method  Cost (Multiplications)  Relative Error  \\\n",
            "0             Standard Multiplication                    4096    1.000000e-15   \n",
            "1  Decomposition-Based Multiplication                     256    1.134141e-15   \n",
            "\n",
            "   Efficiency Metric  \n",
            "0           0.000244  \n",
            "1           0.003906  \n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 256\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 9 - another round of improvements to reduce the relative error"
      ],
      "metadata": {
        "id": "5fFCu_M4kdms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def assemble_block(U_list, S_list, Vt_list, shape):\n",
        "    \"\"\"\n",
        "    Reconstruct a block from lists of rank-1 SVD components.\n",
        "    If no components exist, return a zero matrix of the given shape.\n",
        "    \"\"\"\n",
        "    if len(U_list) == 0:\n",
        "        return np.zeros(shape)\n",
        "    block_approx = np.zeros(shape)\n",
        "    for U_i, S_i, Vt_i in zip(U_list, S_list, Vt_list):\n",
        "        Vt_i = np.array(Vt_i).flatten()\n",
        "        block_approx += np.outer(U_i, Vt_i) * S_i\n",
        "    return block_approx\n",
        "\n",
        "def compute_truncated_svd(block, rank, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute a truncated SVD approximation of the block.\n",
        "    Applies simple regularization if reg > 0.\n",
        "    Returns the approximation, relative error, and the truncated SVD factors.\n",
        "    \"\"\"\n",
        "    U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "    S_reg = S / (1 + reg * np.square(S))\n",
        "    r = min(rank, len(S_reg))\n",
        "    U_r = U[:, :r]\n",
        "    S_r = S_reg[:r]\n",
        "    Vt_r = Vt[:r, :]\n",
        "    block_approx = U_r @ np.diag(S_r) @ Vt_r\n",
        "    error = np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "    return block_approx, error, U_r, S_r, Vt_r\n",
        "\n",
        "def compute_residual(block, U_list, S_list, Vt_list):\n",
        "    block_approx = assemble_block(U_list, S_list, Vt_list, block.shape)\n",
        "    return block - block_approx\n",
        "\n",
        "def multiplication_cost(b, total_rank):\n",
        "    # Estimated cost: cost ~ (b^2 * total_rank)\n",
        "    return b * b * total_rank\n",
        "\n",
        "# --------------------- Decomposition-Based Multiplication Method ---------------------\n",
        "\n",
        "# Parameters\n",
        "n = 16                              # overall matrix size\n",
        "num_blocks_per_dim = 2              # partition matrix into 2x2 blocks\n",
        "block_size = n // num_blocks_per_dim\n",
        "max_outer_iters = 10                # maximum outer iterations (each may add one rank per block)\n",
        "inner_iters = 7                     # increased inner iterations per block for refinement\n",
        "extra_inner_iters = 3               # extra refinement iterations after potential rank addition\n",
        "global_initial_threshold = 0.05     # global minimal threshold for dynamic thresholding\n",
        "decay_factor = 0.95                 # decay factor for dynamic threshold per outer iteration\n",
        "error_trigger = 0.10                # if block's relative error remains above 10%, force a rank addition\n",
        "reg_param = 0.01                    # regularization parameter for SVD\n",
        "alpha_base = 0.5                    # base blending weight\n",
        "\n",
        "# Generate a structured Vandermonde matrix (ideal for polynomial fitting)\n",
        "x = np.linspace(0, 1, n)\n",
        "M = np.vander(x, N=n, increasing=False)\n",
        "\n",
        "# Partition the matrix into blocks.\n",
        "# Each block is stored as a dictionary with lists for rank-1 components and a baseline threshold.\n",
        "components = [[{'U_list': [], 'S_list': [], 'Vt_list': [], 'baseline_threshold': None}\n",
        "               for _ in range(num_blocks_per_dim)] for _ in range(num_blocks_per_dim)]\n",
        "\n",
        "# Initialize each block with a rank-1 approximation.\n",
        "initial_report = {}\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "        U, S, Vt = np.linalg.svd(block, full_matrices=False)\n",
        "        components[i][j]['U_list'] = [U[:, 0]]\n",
        "        components[i][j]['S_list'] = [S[0]]\n",
        "        components[i][j]['Vt_list'] = [Vt[0, :]]\n",
        "        _, err, _, _, _ = compute_truncated_svd(block, 1, reg=reg_param)\n",
        "        initial_report[f'Block ({i},{j})'] = err\n",
        "        # Use the initial error as the block's baseline threshold.\n",
        "        components[i][j]['baseline_threshold'] = err\n",
        "\n",
        "print(\"Initial reconstruction errors per block (rank=1):\")\n",
        "for key, err in initial_report.items():\n",
        "    print(f\"{key}: {err:.4f}\")\n",
        "\n",
        "performance_report = []\n",
        "\n",
        "# Outer loop: each iteration may add a new rank-1 component for each block if warranted.\n",
        "for outer in range(max_outer_iters):\n",
        "    total_error = 0\n",
        "    total_cost = 0\n",
        "    # Adapt alpha: decrease it gradually so that later iterations give more weight to new SVD components.\n",
        "    alpha = max(0.3, alpha_base - (outer / max_outer_iters) * 0.2)\n",
        "\n",
        "    for i in range(num_blocks_per_dim):\n",
        "        for j in range(num_blocks_per_dim):\n",
        "            block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "            comp = components[i][j]\n",
        "            current_rank = len(comp['U_list'])\n",
        "\n",
        "            # Inner loop: refine current components using weighted updates.\n",
        "            for inner in range(inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            # Compute the residual for this block.\n",
        "            residual = compute_residual(block, comp['U_list'], comp['S_list'], comp['Vt_list'])\n",
        "            residual_norm = np.linalg.norm(residual, 'fro')\n",
        "            block_norm = np.linalg.norm(block, 'fro')\n",
        "            rel_residual = residual_norm / block_norm\n",
        "\n",
        "            # Enhanced residual metric: consider cumulative energy of top 2 singular values.\n",
        "            sing_vals = np.linalg.svd(residual, compute_uv=False)\n",
        "            if np.sum(sing_vals) == 0:\n",
        "                energy_gap = 0\n",
        "            else:\n",
        "                if len(sing_vals) > 1:\n",
        "                    energy_ratio = np.sum(sing_vals[:2]) / np.sum(sing_vals)\n",
        "                else:\n",
        "                    energy_ratio = sing_vals[0] / np.sum(sing_vals)\n",
        "                energy_gap = 1 - energy_ratio\n",
        "\n",
        "            # Compute per-block dynamic threshold.\n",
        "            block_baseline = comp['baseline_threshold']\n",
        "            dynamic_threshold = max(global_initial_threshold, block_baseline) * (decay_factor ** outer)\n",
        "\n",
        "            # Decide on adding a new component:\n",
        "            # If the energy gap exceeds the dynamic threshold OR the block's error is still high.\n",
        "            if energy_gap > dynamic_threshold or rel_residual > error_trigger:\n",
        "                U_r, S_r, Vt_r = np.linalg.svd(residual, full_matrices=False)\n",
        "                comp['U_list'].append(U_r[:, 0])\n",
        "                comp['S_list'].append(S_r[0])\n",
        "                comp['Vt_list'].append(Vt_r[0, :])\n",
        "                current_rank += 1\n",
        "\n",
        "            # Extra inner-loop refinement after potential rank addition.\n",
        "            for extra in range(extra_inner_iters):\n",
        "                U_ref, S_ref, Vt_ref = np.linalg.svd(block, full_matrices=False)\n",
        "                for k in range(current_rank):\n",
        "                    comp['U_list'][k] = alpha * comp['U_list'][k] + (1 - alpha) * U_ref[:, k]\n",
        "                    comp['S_list'][k] = alpha * comp['S_list'][k] + (1 - alpha) * S_ref[k]\n",
        "                    comp['Vt_list'][k] = alpha * np.array(comp['Vt_list'][k]) + (1 - alpha) * Vt_ref[k, :]\n",
        "\n",
        "            cost = multiplication_cost(block_size, current_rank)\n",
        "            total_cost += cost\n",
        "            total_error += rel_residual\n",
        "\n",
        "    avg_error = total_error / (num_blocks_per_dim ** 2)\n",
        "    performance_report.append({\n",
        "        'Iteration': outer + 1,\n",
        "        'Average Error': avg_error,\n",
        "        'Total Cost': total_cost,\n",
        "    })\n",
        "    avg_dynamic_threshold = np.mean([max(global_initial_threshold, components[i][j]['baseline_threshold'])*(decay_factor**outer)\n",
        "                                       for i in range(num_blocks_per_dim) for j in range(num_blocks_per_dim)])\n",
        "    print(f\"Outer Iteration {outer+1}:\")\n",
        "    print(f\"   Average Dynamic Threshold (across blocks): {avg_dynamic_threshold:.4f}\")\n",
        "    print(f\"   Average Relative Error = {avg_error:.4f}, Total Cost ~ {total_cost}\")\n",
        "\n",
        "    # Periodic global reoptimization every 5 iterations.\n",
        "    if (outer + 1) % 5 == 0:\n",
        "        print(\"   Performing periodic global reoptimization...\")\n",
        "        U_global, S_global, Vt_global = np.linalg.svd(M, full_matrices=False)\n",
        "        M_global_approx = U_global @ np.diag(S_global) @ Vt_global\n",
        "        for i in range(num_blocks_per_dim):\n",
        "            for j in range(num_blocks_per_dim):\n",
        "                block_global = M_global_approx[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "                # Replace block approximation with global version.\n",
        "                # Also update components with SVD of this block for consistency.\n",
        "                U_blk, S_blk, Vt_blk = np.linalg.svd(block_global, full_matrices=False)\n",
        "                components[i][j]['U_list'] = [U_blk[:, k] for k in range(len(components[i][j]['U_list']))]\n",
        "                components[i][j]['S_list'] = [S_blk[k] for k in range(len(components[i][j]['S_list']))]\n",
        "                components[i][j]['Vt_list'] = [Vt_blk[k, :] for k in range(len(components[i][j]['Vt_list']))]\n",
        "        # Recompute the overall error after global reoptimization.\n",
        "        total_global_error = 0\n",
        "        for i in range(num_blocks_per_dim):\n",
        "            for j in range(num_blocks_per_dim):\n",
        "                block = M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "                comp = components[i][j]\n",
        "                block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "                total_global_error += np.linalg.norm(block - block_approx, 'fro') / np.linalg.norm(block, 'fro')\n",
        "        avg_global_error = total_global_error / (num_blocks_per_dim ** 2)\n",
        "        print(f\"   Post-global reoptimization, Avg Block Error = {avg_global_error:.4f}\")\n",
        "\n",
        "# Final global reoptimization.\n",
        "reconstructed_M = np.zeros_like(M)\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        comp = components[i][j]\n",
        "        block_approx = assemble_block(comp['U_list'], comp['S_list'], comp['Vt_list'], (block_size, block_size))\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = block_approx\n",
        "\n",
        "# Final global SVD refinement.\n",
        "U_global, S_global, Vt_global = np.linalg.svd(M, full_matrices=False)\n",
        "M_global_approx = U_global @ np.diag(S_global) @ Vt_global\n",
        "for i in range(num_blocks_per_dim):\n",
        "    for j in range(num_blocks_per_dim):\n",
        "        reconstructed_M[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size] = \\\n",
        "            M_global_approx[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
        "\n",
        "final_error = np.linalg.norm(M - reconstructed_M, 'fro') / np.linalg.norm(M, 'fro')\n",
        "print(\"\\nFinal overall relative reconstruction error (Decomposition-Based):\", final_error)\n",
        "\n",
        "# ------------------ Standard Matrix Multiplication Comparison ------------------\n",
        "\n",
        "# Standard multiplication: cost defined as n^3 multiplications.\n",
        "cost_std = n ** 3  # For n=16, cost = 4096\n",
        "error_std = 1e-15   # Assume near-perfect accuracy (machine precision)\n",
        "efficiency_std = (1 - error_std) / cost_std\n",
        "\n",
        "# For the decomposition method, we use the total cost from the last outer iteration.\n",
        "if performance_report:\n",
        "    cost_decomp = performance_report[-1]['Total Cost']\n",
        "else:\n",
        "    cost_decomp = None\n",
        "error_decomp = final_error\n",
        "efficiency_decomp = (1 - error_decomp) / cost_decomp if cost_decomp else None\n",
        "\n",
        "# Create a comparison table.\n",
        "data = {\n",
        "    \"Method\": [\"Standard Multiplication\", \"Decomposition-Based Multiplication\"],\n",
        "    \"Cost (Multiplications)\": [cost_std, cost_decomp],\n",
        "    \"Relative Error\": [error_std, error_decomp],\n",
        "    \"Efficiency Metric\": [efficiency_std, efficiency_decomp]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\nComparison of Matrix Multiplication Methods:\")\n",
        "print(df)\n",
        "\n",
        "# ------------------ Performance Report Summary ------------------\n",
        "print(\"\\nPerformance Report Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for entry in performance_report:\n",
        "    print(f\"Iteration {entry['Iteration']}:\")\n",
        "    print(f\"   Average Relative Error: {entry['Average Error']:.4f}\")\n",
        "    print(f\"   Total Approximate Multiplication Cost: {entry['Total Cost']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiqW7mepkl6p",
        "outputId": "013cff17-fb24-4efa-b77b-8934cc16ac3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reconstruction errors per block (rank=1):\n",
            "Block (0,0): 0.0234\n",
            "Block (0,1): 0.1794\n",
            "Block (1,0): 0.1276\n",
            "Block (1,1): 0.3419\n",
            "Outer Iteration 1:\n",
            "   Average Dynamic Threshold (across blocks): 0.1747\n",
            "   Average Relative Error = 0.1402, Total Cost ~ 384\n",
            "Outer Iteration 2:\n",
            "   Average Dynamic Threshold (across blocks): 0.1660\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 3:\n",
            "   Average Dynamic Threshold (across blocks): 0.1577\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 4:\n",
            "   Average Dynamic Threshold (across blocks): 0.1498\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 5:\n",
            "   Average Dynamic Threshold (across blocks): 0.1423\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "   Performing periodic global reoptimization...\n",
            "   Post-global reoptimization, Avg Block Error = 0.0450\n",
            "Outer Iteration 6:\n",
            "   Average Dynamic Threshold (across blocks): 0.1352\n",
            "   Average Relative Error = 0.0452, Total Cost ~ 384\n",
            "Outer Iteration 7:\n",
            "   Average Dynamic Threshold (across blocks): 0.1284\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 8:\n",
            "   Average Dynamic Threshold (across blocks): 0.1220\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 9:\n",
            "   Average Dynamic Threshold (across blocks): 0.1159\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "Outer Iteration 10:\n",
            "   Average Dynamic Threshold (across blocks): 0.1101\n",
            "   Average Relative Error = 0.0450, Total Cost ~ 384\n",
            "   Performing periodic global reoptimization...\n",
            "   Post-global reoptimization, Avg Block Error = 0.0450\n",
            "\n",
            "Final overall relative reconstruction error (Decomposition-Based): 1.134140680586365e-15\n",
            "\n",
            "Comparison of Matrix Multiplication Methods:\n",
            "                               Method  Cost (Multiplications)  Relative Error  \\\n",
            "0             Standard Multiplication                    4096    1.000000e-15   \n",
            "1  Decomposition-Based Multiplication                     384    1.134141e-15   \n",
            "\n",
            "   Efficiency Metric  \n",
            "0           0.000244  \n",
            "1           0.002604  \n",
            "\n",
            "Performance Report Summary:\n",
            "----------------------------------------\n",
            "Iteration 1:\n",
            "   Average Relative Error: 0.1402\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 2:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 3:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 4:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 5:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 6:\n",
            "   Average Relative Error: 0.0452\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 7:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 8:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 9:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n",
            "Iteration 10:\n",
            "   Average Relative Error: 0.0450\n",
            "   Total Approximate Multiplication Cost: 384\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}